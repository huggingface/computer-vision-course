# Vision Transformers å¯¹è±¡æ£€æµ‹

æœ¬ç« èŠ‚å°†ä»‹ç»å¦‚ä½•ä½¿ç”¨ Vision Transformers æ¥å®ç°å¯¹è±¡æ£€æµ‹ä»»åŠ¡ã€‚æˆ‘ä»¬å°†å­¦ä¹ å¦‚ä½•é’ˆå¯¹æˆ‘ä»¬çš„ç”¨ä¾‹å¾®è°ƒç°æœ‰çš„é¢„è®­ç»ƒå¯¹è±¡æ£€æµ‹æ¨¡å‹ã€‚å¼€å§‹ä¹‹å‰ï¼Œå»ºè®®è®¿é—®è¿™ä¸ª HuggingFace Spaceï¼Œæ‚¨å¯ä»¥å°è¯•ä¸æœ€ç»ˆç»“æœäº’åŠ¨ã€‚

<iframe
	src="https://hf-vision-finetuning-demo-for-object-detection.hf.space/"
	frameborder="0"
	width="850"
	height="450">
</iframe>

## ä»‹ç»

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/object_detection_wiki.png" alt="å¯¹è±¡æ£€æµ‹ç¤ºä¾‹">
<p>å›¾1ï¼šå¯¹è±¡æ£€æµ‹ç¤ºä¾‹</p>
</div>

å¯¹è±¡æ£€æµ‹æ˜¯è®¡ç®—æœºè§†è§‰ä¸­çš„ä¸€é¡¹ä»»åŠ¡ï¼Œæ¶‰åŠè¯†åˆ«å’Œå®šä½å›¾åƒæˆ–è§†é¢‘ä¸­çš„å¯¹è±¡ã€‚å®ƒåŒ…å«ä¸¤ä¸ªä¸»è¦æ­¥éª¤ï¼š

- é¦–å…ˆï¼Œè¯†åˆ«å­˜åœ¨çš„å¯¹è±¡ç±»å‹ï¼ˆä¾‹å¦‚æ±½è½¦ã€äººç‰©æˆ–åŠ¨ç‰©ï¼‰ã€‚
- å…¶æ¬¡ï¼Œé€šè¿‡ç»˜åˆ¶è¾¹ç•Œæ¡†ç¡®å®šå…¶ç²¾ç¡®ä½ç½®ã€‚

è¿™äº›æ¨¡å‹é€šå¸¸æ¥æ”¶å›¾åƒï¼ˆé™æ€å›¾åƒæˆ–è§†é¢‘å¸§ï¼‰ä½œä¸ºè¾“å…¥ï¼Œæ¯å¹…å›¾åƒä¸­å¯èƒ½åŒ…å«å¤šä¸ªå¯¹è±¡ã€‚ä¾‹å¦‚ï¼Œè€ƒè™‘åŒ…å«æ±½è½¦ã€äººç‰©ã€è‡ªè¡Œè½¦ç­‰å¤šä¸ªå¯¹è±¡çš„å›¾åƒã€‚å¤„ç†è¾“å…¥åï¼Œè¿™äº›æ¨¡å‹ä¼šç”Ÿæˆä¸€ç»„æ•°å€¼ï¼Œä¼ è¾¾ä»¥ä¸‹ä¿¡æ¯ï¼š

- å¯¹è±¡çš„ä½ç½®ï¼ˆè¾¹ç•Œæ¡†çš„ XY åæ ‡ï¼‰ã€‚
- å¯¹è±¡çš„ç±»åˆ«ã€‚

å¯¹è±¡æ£€æµ‹æœ‰è®¸å¤šåº”ç”¨ï¼Œå…¶ä¸­ä¸€ä¸ªæ˜¾è‘—çš„ä¾‹å­æ˜¯è‡ªåŠ¨é©¾é©¶é¢†åŸŸã€‚å¯¹è±¡æ£€æµ‹ç”¨äºæ£€æµ‹æ±½è½¦å‘¨å›´çš„ä¸åŒå¯¹è±¡ï¼ˆå¦‚è¡Œäººã€è·¯æ ‡ã€äº¤é€šä¿¡å·ç¯ç­‰ï¼‰ï¼Œå¹¶æˆä¸ºå†³ç­–çš„è¾“å…¥ä¹‹ä¸€ã€‚

å¦‚éœ€æ·±å…¥äº†è§£å¯¹è±¡æ£€æµ‹çš„è¯¦ç»†å†…å®¹ï¼Œè¯·æŸ¥çœ‹æˆ‘ä»¬ä¸“é—¨çš„[ç« èŠ‚](https://huggingface.co/learn/computer-vision-course/unit6/basic-cv-tasks/object_detection)ğŸ¤—ã€‚

### å¾®è°ƒå¯¹è±¡æ£€æµ‹æ¨¡å‹çš„å¿…è¦æ€§ ğŸ¤”

è¿™æ˜¯ä¸€ä¸ªå¾ˆæ£’çš„é—®é¢˜ã€‚ä»å¤´å¼€å§‹è®­ç»ƒå¯¹è±¡æ£€æµ‹æ¨¡å‹æ„å‘³ç€ï¼š

- ä¸æ–­é‡å¤å·²ç»å®Œæˆçš„ç ”ç©¶ã€‚
- ç¼–å†™é‡å¤çš„æ¨¡å‹ä»£ç ã€è®­ç»ƒæ¨¡å‹ï¼Œå¹¶ä¸ºä¸åŒçš„ç”¨ä¾‹ç»´æŠ¤ä¸åŒçš„ä»£ç åº“ã€‚
- éœ€è¦å¤§é‡çš„å®éªŒå¹¶æµªè´¹èµ„æºã€‚

ä¸å…¶è¿›è¡Œè¿™äº›é‡å¤æ“ä½œï¼Œä¸å¦‚é‡‡ç”¨ä¸€ä¸ªè¡¨ç°å‡ºè‰²çš„é¢„è®­ç»ƒæ¨¡å‹ï¼ˆä¸€ä¸ªåœ¨è¯†åˆ«ä¸€èˆ¬ç‰¹å¾æ–¹é¢è¡¨ç°ä¼˜å¼‚çš„æ¨¡å‹ï¼‰ï¼Œå¹¶è°ƒæ•´æˆ–é‡æ–°å¾®è°ƒå…¶æƒé‡ï¼ˆæˆ–éƒ¨åˆ†æƒé‡ï¼‰ï¼Œä»¥é€‚åº”ç‰¹å®šçš„ç”¨ä¾‹ã€‚æˆ‘ä»¬è®¤ä¸ºï¼Œé¢„è®­ç»ƒæ¨¡å‹å·²ç»å­¦ä¼šäº†è¶³å¤Ÿå¤šçš„ç‰¹å¾ï¼Œå¯ä»¥åœ¨å›¾åƒä¸­å®šä½å’Œåˆ†ç±»å¯¹è±¡ã€‚å› æ­¤ï¼Œå¦‚æœå¼•å…¥æ–°å¯¹è±¡ï¼Œåˆ©ç”¨å·²ç»å­¦ä¹ çš„ç‰¹å¾å’Œæ–°çš„ç‰¹å¾ï¼Œå¯ä»¥é€šè¿‡å°‘é‡æ—¶é—´å’Œè®¡ç®—æ¥è®­ç»ƒç›¸åŒçš„æ¨¡å‹ï¼Œä»è€Œå¼€å§‹æ£€æµ‹è¿™äº›æ–°å¯¹è±¡ã€‚

é€šè¿‡æœ¬æ•™ç¨‹çš„å­¦ä¹ ï¼Œæ‚¨å°†èƒ½å¤Ÿåˆ›å»ºå®Œæ•´çš„å¯¹è±¡æ£€æµ‹æµæ°´çº¿ï¼ˆåŒ…æ‹¬åŠ è½½æ•°æ®é›†ã€å¾®è°ƒæ¨¡å‹å’Œè¿›è¡Œæ¨æ–­ï¼‰ã€‚

## å®‰è£…å¿…è¦çš„åº“

é¦–å…ˆè¿›è¡Œå®‰è£…ã€‚æ‰§è¡Œä»¥ä¸‹å‘½ä»¤æ¥å®‰è£…å¿…è¦çš„è½¯ä»¶åŒ…ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨ Hugging Face Transformers å’Œ PyTorchã€‚

```bash
!pip install -U -q datasets transformers[torch] evaluate timm albumentations accelerate
```

## åœºæ™¯

ä¸ºäº†è®©æœ¬æ•™ç¨‹æ›´å…·å¸å¼•åŠ›ï¼Œè®©æˆ‘ä»¬è€ƒè™‘ä¸€ä¸ªç°å®çš„ç¤ºä¾‹ï¼šå»ºç­‘å·¥äººéœ€è¦åœ¨æ–½å·¥åŒºåŸŸä¿æŒæœ€é«˜çš„å®‰å…¨æ€§ã€‚åŸºæœ¬çš„å®‰å…¨åè®®è¦æ±‚éšæ—¶ä½©æˆ´å¤´ç›”ã€‚ç”±äºæœ‰å¾ˆå¤šå»ºç­‘å·¥äººï¼Œå¾ˆéš¾æ—¶åˆ»ç›‘æ§æ¯ä¸ªäººã€‚

ä½†å¦‚æœæˆ‘ä»¬èƒ½å¤Ÿæ‹¥æœ‰ä¸€ä¸ªå¯ä»¥å®æ—¶æ£€æµ‹æ˜¯å¦ä½©æˆ´å¤´ç›”çš„æ‘„åƒç³»ç»Ÿï¼Œé‚£å°±éå¸¸ç†æƒ³äº†ï¼Œå¯¹å—ï¼Ÿ

å› æ­¤ï¼Œæˆ‘ä»¬å°†å¾®è°ƒä¸€ä¸ªè½»é‡çº§çš„å¯¹è±¡æ£€æµ‹æ¨¡å‹ï¼Œä»¥å®ç°è¿™ä¸€ç›®æ ‡ã€‚è®©æˆ‘ä»¬å¼€å§‹å§ã€‚


### æ•°æ®é›†

åœ¨ä¸Šè¿°åœºæ™¯ä¸­ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨[Northeaster University China](https://dataverse.harvard.edu/dataset.xhtml?persistentId=doi:10.7910/DVN/7CBGOS)æä¾›çš„[hardhat](https://huggingface.co/datasets/hf-vision/hardhat)æ•°æ®é›†ã€‚æˆ‘ä»¬å¯ä»¥ä½¿ç”¨ ğŸ¤— `datasets` ä¸‹è½½å¹¶åŠ è½½æ­¤æ•°æ®é›†ã€‚

```python
from datasets import load_dataset

dataset = load_dataset("anindya64/hardhat")
dataset
```

è¿™å°†æä¾›ä»¥ä¸‹æ•°æ®ç»“æ„ï¼š

```
DatasetDict({
    train: Dataset({
        features: ['image', 'image_id', 'width', 'height', 'objects'],
        num_rows: 5297
    })
    test: Dataset({
        features: ['image', 'image_id', 'width', 'height', 'objects'],
        num_rows: 1766
    })
})
```

ä»¥ä¸Šæ˜¯ä¸€ä¸ª[DatasetDict](https://huggingface.co/docs/datasets/v2.17.1/en/package_reference/main_classes#datasets.DatasetDict)ï¼Œè¿™æ˜¯ä¸€ä¸ªåŒ…å«æ•´ä¸ªæ•°æ®é›†ï¼ˆæŒ‰è®­ç»ƒé›†å’Œæµ‹è¯•é›†åˆ’åˆ†ï¼‰çš„é«˜æ•ˆå­—å…¸ç»“æ„ã€‚å¦‚æ‚¨æ‰€è§ï¼Œåœ¨æ¯ä¸ªåˆ’åˆ†ï¼ˆè®­ç»ƒé›†å’Œæµ‹è¯•é›†ï¼‰ä¸‹ï¼Œæˆ‘ä»¬æœ‰`features`å’Œ`num_rows`ã€‚åœ¨`features`ä¸­ï¼Œæœ‰`image`ï¼ˆä¸€ä¸ª[Pillowå¯¹è±¡](https://realpython.com/image-processing-with-the-python-pillow-library/)ï¼‰ã€å›¾åƒçš„IDã€é«˜åº¦å’Œå®½åº¦ä»¥åŠ`objects`ã€‚  
ç°åœ¨è®©æˆ‘ä»¬çœ‹çœ‹æ¯ä¸ªæ•°æ®ç‚¹ï¼ˆåœ¨è®­ç»ƒ/æµ‹è¯•é›†ä¸­ï¼‰æ˜¯ä»€ä¹ˆæ ·çš„ã€‚è¦åšåˆ°è¿™ä¸€ç‚¹ï¼Œè¿è¡Œä»¥ä¸‹ä»£ç ï¼š

```python
dataset["train"][0]
```

è¿™å°†ç”Ÿæˆä»¥ä¸‹ç»“æ„ï¼š

```
{'image': <PIL.JpegImagePlugin.JpegImageFile image mode=RGB size=500x375>,
 'image_id': 1,
 'width': 500,
 'height': 375,
 'objects': {'id': [1, 1],
  'area': [3068.0, 690.0],
  'bbox': [[178.0, 84.0, 52.0, 59.0], [111.0, 144.0, 23.0, 30.0]],
  'category': ['helmet', 'helmet']}}
```

å¦‚æ‚¨æ‰€è§ï¼Œ`objects`æ˜¯å¦ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«å¯¹è±¡IDï¼ˆå³ç±»åˆ«IDï¼‰ã€å¯¹è±¡çš„é¢ç§¯ä»¥åŠè¾¹ç•Œæ¡†åæ ‡(`bbox`)å’Œç±»åˆ«ï¼ˆæ ‡ç­¾ï¼‰ã€‚ä»¥ä¸‹æ˜¯æ¯ä¸ªé”®å’Œæ•°æ®å…ƒç´ å€¼çš„è¯¦ç»†è§£é‡Šã€‚

- `image`: è¿™æ˜¯ä¸€ä¸ªPillowå›¾åƒå¯¹è±¡ï¼Œå¯ç›´æ¥åœ¨åŠ è½½è·¯å¾„ä¹‹å‰æŸ¥çœ‹å›¾åƒã€‚
- `image_id`: è¡¨ç¤ºå›¾åƒåœ¨è®­ç»ƒæ–‡ä»¶ä¸­çš„ç¼–å·ã€‚
- `width`: å›¾åƒçš„å®½åº¦ã€‚
- `height`: å›¾åƒçš„é«˜åº¦ã€‚
- `objects`: å¦ä¸€ä¸ªåŒ…å«æ³¨é‡Šä¿¡æ¯çš„å­—å…¸ã€‚åŒ…å«ä»¥ä¸‹å†…å®¹ï¼š
    - `id`: ä¸€ä¸ªåˆ—è¡¨ï¼Œåˆ—è¡¨çš„é•¿åº¦è¡¨ç¤ºå¯¹è±¡çš„æ•°é‡ï¼Œæ¯ä¸ªå€¼è¡¨ç¤ºç±»ç´¢å¼•ã€‚
    - `area`: å¯¹è±¡çš„é¢ç§¯ã€‚
    - `bbox`: è¡¨ç¤ºå¯¹è±¡çš„è¾¹ç•Œæ¡†åæ ‡ã€‚
    - `category`: å¯¹è±¡çš„ç±»ï¼ˆå­—ç¬¦ä¸²ï¼‰ã€‚

ç°åœ¨æˆ‘ä»¬æ¥æ­£ç¡®æå–è®­ç»ƒå’Œæµ‹è¯•æ ·æœ¬ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬æœ‰å¤§çº¦5000ä¸ªè®­ç»ƒæ ·æœ¬å’Œ1700ä¸ªæµ‹è¯•æ ·æœ¬ã€‚

```python
# é¦–å…ˆï¼Œæå–è®­ç»ƒé›†å’Œæµ‹è¯•é›†

train_dataset = dataset["train"]
test_dataset = dataset["test"]
```

ç°åœ¨æˆ‘ä»¬çŸ¥é“æ ·æœ¬æ•°æ®ç‚¹çš„å†…å®¹ï¼Œè®©æˆ‘ä»¬å¼€å§‹ç»˜åˆ¶è¯¥æ ·æœ¬ã€‚æˆ‘ä»¬å°†é¦–å…ˆç»˜åˆ¶å›¾åƒï¼Œç„¶åç»˜åˆ¶ç›¸åº”çš„è¾¹ç•Œæ¡†ã€‚

ä»¥ä¸‹æ˜¯æˆ‘ä»¬è¦åšçš„æ­¥éª¤ï¼š

1. è·å–å›¾åƒåŠå…¶ç›¸åº”çš„é«˜åº¦å’Œå®½åº¦ã€‚
2. åˆ›å»ºä¸€ä¸ªå¯åœ¨å›¾åƒä¸Šç»˜åˆ¶æ–‡æœ¬å’Œçº¿æ¡çš„ç»˜åˆ¶å¯¹è±¡ã€‚
3. ä»æ ·æœ¬ä¸­è·å–æ³¨é‡Šå­—å…¸ã€‚
4. éå†æ³¨é‡Šå­—å…¸ã€‚
5. å¯¹äºæ¯ä¸ªå¯¹è±¡ï¼Œè·å–è¾¹ç•Œæ¡†åæ ‡ï¼Œå³xï¼ˆæ°´å¹³èµ·å§‹ä½ç½®ï¼‰ã€yï¼ˆå‚ç›´èµ·å§‹ä½ç½®ï¼‰ã€wï¼ˆè¾¹ç•Œæ¡†çš„å®½åº¦ï¼‰ã€hï¼ˆè¾¹ç•Œæ¡†çš„é«˜åº¦ï¼‰ã€‚
6. å¦‚æœè¾¹ç•Œæ¡†çš„åº¦é‡æ˜¯å½’ä¸€åŒ–çš„ï¼Œåˆ™è¿›è¡Œç¼©æ”¾ï¼Œå¦åˆ™ä¿æŒä¸å˜ã€‚
7. æœ€åç»˜åˆ¶çŸ©å½¢å’Œç±»åˆ«æ–‡æœ¬ã€‚

```python 
import numpy as np
from PIL import Image, ImageDraw


def draw_image_from_idx(dataset, idx):
    sample = dataset[idx]
    image = sample["image"]
    annotations = sample["objects"]
    draw = ImageDraw.Draw(image)
    width, height = sample["width"], sample["height"]

    for i in range(len(annotations["id"])):
        box = annotations["bbox"][i]
        class_idx = annotations["id"][i]
        x, y, w, h = tuple(box)
        if max(box) > 1.0:
            x1, y1 = int(x), int(y)
            x2, y2 = int(x + w), int(y + h)
        else:
            x1 = int(x * width)
            y1 = int(y * height)
            x2 = int((x + w) * width)
            y2 = int((y + h) * height)
        draw.rectangle((x1, y1, x2, y2), outline="red", width=1)
        draw.text((x1, y1), annotations["category"][i], fill="white")
    return image


draw_image_from_idx(dataset=train_dataset, idx=10)
```

æˆ‘ä»¬æœ‰ä¸€ä¸ªå‡½æ•°æ¥ç»˜åˆ¶å•ä¸ªå›¾åƒï¼Œæ¥ä¸‹æ¥ç¼–å†™ä¸€ä¸ªç®€å•çš„å‡½æ•°æ¥ä½¿ç”¨ä¸Šè¿°ä»£ç ç»˜åˆ¶å¤šä¸ªå›¾åƒã€‚è¿™æ ·å¯ä»¥å¸®åŠ©æˆ‘ä»¬è¿›è¡Œä¸€äº›åˆ†æã€‚

```python
import matplotlib.pyplot as plt


def plot_images(dataset, indices):
    """
    ç»˜åˆ¶å›¾åƒåŠå…¶æ³¨é‡Šã€‚
    """
    num_rows = len(indices) // 3
    num_cols = 3
    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 10))

    for i, idx in enumerate(indices):
        row = i // num_cols
        col = i % num_cols

        # ç»˜åˆ¶å›¾åƒ
        image = draw_image_from_idx(dataset, idx)

        # åœ¨å¯¹åº”çš„å­å›¾ä¸Šæ˜¾ç¤ºå›¾åƒ
        axes[row, col].imshow(image)
        axes[row, col].axis("off")

    plt.tight_layout()
    plt.show()


# ç°åœ¨ä½¿ç”¨è¯¥å‡½æ•°ç»˜åˆ¶å›¾åƒ

plot_images(train_dataset, range(9))
```

è¿è¡Œæ­¤å‡½æ•°å°†ç”Ÿæˆä¸€ä¸ªæ¼‚äº®çš„æ‹¼å›¾ï¼Œå¦‚ä¸‹æ‰€ç¤ºã€‚

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/object_detection_train_image_with_annotation_plots.png" alt="input-image-plot">
<p>å›¾2ï¼šinput-image-plot</p>
</div>

## AutoImageProcessor

åœ¨å¯¹æ¨¡å‹è¿›è¡Œå¾®è°ƒä¹‹å‰ï¼Œæˆ‘ä»¬å¿…é¡»å¯¹æ•°æ®è¿›è¡Œé¢„å¤„ç†ï¼Œä»¥ç¡®ä¿ä¸é¢„è®­ç»ƒæ—¶çš„æ–¹æ³•å®Œå…¨åŒ¹é…ã€‚HuggingFace çš„ [AutoImageProcessor](https://huggingface.co/docs/transformers/v4.36.0/en/model_doc/auto#transformers.AutoImageProcessor) è´Ÿè´£å¤„ç†å›¾åƒæ•°æ®ï¼Œä»¥ç”Ÿæˆ `pixel_values`ã€`pixel_mask` å’Œ `labels`ï¼Œä¾› DETR æ¨¡å‹è¿›è¡Œè®­ç»ƒã€‚

ç°åœ¨ï¼Œè®©æˆ‘ä»¬ä»æˆ‘ä»¬å¸Œæœ›ç”¨æ¥å¾®è°ƒæ¨¡å‹çš„ç›¸åŒæ£€æŸ¥ç‚¹å®ä¾‹åŒ–å›¾åƒå¤„ç†å™¨ã€‚

```python
from transformers import AutoImageProcessor

checkpoint = "facebook/detr-resnet-50-dc5"
image_processor = AutoImageProcessor.from_pretrained(checkpoint)
```

## æ•°æ®é›†é¢„å¤„ç†

åœ¨å°†å›¾åƒä¼ é€’ç»™ `image_processor` ä¹‹å‰ï¼Œè®©æˆ‘ä»¬è¿˜å¯¹å›¾åƒåŠå…¶å¯¹åº”çš„è¾¹ç•Œæ¡†åº”ç”¨ä¸åŒç±»å‹çš„å¢å¼ºã€‚

ç®€å•æ¥è¯´ï¼Œå¢å¼ºæ˜¯ä¸€ç»„éšæœºè½¬æ¢ï¼Œå¦‚æ—‹è½¬ã€è°ƒæ•´å¤§å°ç­‰ã€‚è¿™äº›è½¬æ¢è¢«åº”ç”¨äºå›¾åƒï¼Œä»¥è·å¾—æ›´å¤šæ ·æœ¬ï¼Œå¹¶ä½¿è§†è§‰æ¨¡å‹åœ¨ä¸åŒå›¾åƒæ¡ä»¶ä¸‹æ›´åŠ å¥å£®ã€‚æˆ‘ä»¬å°†ä½¿ç”¨ [albumentations](https://github.com/albumentations-team/albumentations) åº“æ¥å®ç°è¿™ä¸€ç‚¹ã€‚å®ƒå…è®¸æ‚¨åˆ›å»ºå›¾åƒçš„éšæœºå˜æ¢ï¼Œä»è€Œå¢åŠ è®­ç»ƒæ ·æœ¬æ•°é‡ã€‚

```python
import albumentations
import numpy as np
import torch

transform = albumentations.Compose(
    [
        albumentations.Resize(480, 480),
        albumentations.HorizontalFlip(p=1.0),
        albumentations.RandomBrightnessContrast(p=1.0),
    ],
    bbox_params=albumentations.BboxParams(format="coco", label_fields=["category"]),
)
```

ä¸€æ—¦æˆ‘ä»¬åˆå§‹åŒ–äº†æ‰€æœ‰çš„è½¬æ¢ï¼Œæˆ‘ä»¬éœ€è¦åˆ›å»ºä¸€ä¸ªå‡½æ•°æ¥æ ¼å¼åŒ–æ³¨é‡Šï¼Œå¹¶è¿”å›ç‰¹å®šæ ¼å¼çš„æ³¨é‡Šåˆ—è¡¨ã€‚

è¿™æ˜¯å› ä¸º `image_processor` æœŸæœ›æ³¨é‡Šçš„æ ¼å¼å¦‚ä¸‹ï¼š`{'image_id': int, 'annotations': List[Dict]}`ï¼Œå…¶ä¸­æ¯ä¸ªå­—å…¸æ˜¯ä¸€ä¸ª COCO å¯¹è±¡æ³¨é‡Šã€‚

```python
def formatted_anns(image_id, category, area, bbox):
    annotations = []
    for i in range(0, len(category)):
        new_ann = {
            "image_id": image_id,
            "category_id": category[i],
            "isCrowd": 0,
            "area": area[i],
            "bbox": list(bbox[i]),
        }
        annotations.append(new_ann)

    return annotations
```

æœ€åï¼Œæˆ‘ä»¬å°†å›¾åƒå’Œæ³¨é‡Šçš„è½¬æ¢ç»“åˆèµ·æ¥ï¼Œå¯¹æ•´ä¸ªæ•°æ®é›†æ‰¹é‡è¿›è¡Œè½¬æ¢ã€‚

ä»¥ä¸‹æ˜¯æ‰§è¡Œæ­¤æ“ä½œçš„æœ€ç»ˆä»£ç ï¼š

```python
# æ‰¹é‡è½¬æ¢

def transform_aug_ann(examples):
    image_ids = examples["image_id"]
    images, bboxes, area, categories = [], [], [], []
    for image, objects in zip(examples["image"], examples["objects"]):
        image = np.array(image.convert("RGB"))[:, :, ::-1]
        out = transform(image=image, bboxes=objects["bbox"], category=objects["id"])

        area.append(objects["area"])
        images.append(out["image"])
        bboxes.append(out["bboxes"])
        categories.append(out["category"])

    targets = [
        {"image_id": id_, "annotations": formatted_anns(id_, cat_, ar_, box_)}
        for id_, cat_, ar_, box_ in zip(image_ids, categories, area, bboxes)
    ]

    return image_processor(images=images, annotations=targets, return_tensors="pt")
```

æœ€åï¼Œä½ åªéœ€å°†æ­¤é¢„å¤„ç†å‡½æ•°åº”ç”¨äºæ•´ä¸ªæ•°æ®é›†ã€‚ä½ å¯ä»¥é€šè¿‡ä½¿ç”¨ HuggingFace ğŸ¤— çš„ [Datasets with transform](https://huggingface.co/docs/datasets/v2.15.0/en/package_reference/main_classes#datasets.Dataset.with_transform) æ–¹æ³•æ¥å®ç°ã€‚

```python
# å¯¹è®­ç»ƒé›†å’Œæµ‹è¯•é›†è¿›è¡Œè½¬æ¢

train_dataset_transformed = train_dataset.with_transform(transform_aug_ann)
test_dataset_transformed = test_dataset.with_transform(transform_aug_ann)
```

ç°åœ¨ï¼Œè®©æˆ‘ä»¬çœ‹çœ‹è½¬æ¢åçš„è®­ç»ƒé›†æ ·æœ¬æ˜¯ä»€ä¹ˆæ ·çš„ï¼š

```python
train_dataset_transformed[0]
```

è¿™å°†è¿”å›ä¸€ä¸ªå¼ é‡å­—å…¸ã€‚æˆ‘ä»¬ä¸»è¦éœ€è¦çš„æ˜¯ä»£è¡¨å›¾åƒçš„ `pixel_values`ã€ä½œä¸ºæ³¨æ„åŠ›æ©ç çš„ `pixel_mask` å’Œæ ‡ç­¾ `labels`ã€‚ä»¥ä¸‹æ˜¯ä¸€ä¸ªæ•°æ®ç‚¹çš„ç¤ºä¾‹ï¼š

```
{'pixel_values': tensor([[[-0.1657, -0.1657, -0.1657,  ..., -0.3369, -0.4739, -0.5767],
          [-0.1657, -0.1657, -0.1657,  ..., -0.3369, -0.4739, -0.5767],
          [-0.1657, -0.1657, -0.1828,  ..., -0.3541, -0.4911, -0.5938],
          ...,
          [-0.4911, -0.5596, -0.6623,  ..., -0.7137, -0.7650, -0.7993],
          [-0.4911, -0.5596, -0.6794,  ..., -0.7308, -0.7993, -0.8335],
          [-0.4911, -0.5596, -0.6794,  ..., -0.7479, -0.8164, -0.8507]],
 
         [[-0.0924, -0.0924, -0.0924,  ...,  0.0651, -0.0749, -0.1800],
          [-0.0924, -0.0924, -0.0924,  ...,  0.0651, -0.0924, -0.2150],
          [-0.0924, -0.0924, -0.1099,  ...,  0.0476, -0.1275, -0.2500],
          ...,
          [-0.0924, -0.1800, -0.3200,  ..., -0.4426, -0.4951, -0.5301],
          [-0.0924, -0.1800, -0.3200,  ..., -0.4601, -0.5126, -0.5651],
          [-0.0924, -0.1800, -0.3200,  ..., -0.4601, -0.5301, -0.5826]],
 
         [[ 0.1999,  0.1999,  0.1999,  ...,  0.6705,  0.5136,  0.4091],
          [ 0.1999,  0.1999,  0.1999,  ...,  0.6531,  0.4962,  0.3916],
          [ 0.1999,  0.1999,  0.1825,  ...,  0.6356,  0.4614,  0.3568],
          ...,
          [ 0.4788,  0.3916,  0.2696,  ...,  0.1825,  0.1302,  0.0953],
          [ 0.4788,  0.3916,  0.2696,  ...,  0.1651,  0.0953,  0.0605],
          [ 0.4788,  0.3916,  0.2696,  ...,  0.1476,  0.0779,  0.0431]]]),
 'pixel_mask': tensor([[1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         ...,
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1],
         [1, 1, 1,  ..., 1, 1, 1]]),
 'labels': {'size': tensor([800, 800]), 'image_id': tensor([1]), 'class_labels': tensor([1, 1]), 'boxes': tensor([[0.5920, 0.3027, 0.1040, 0.1573],
         [0.7550, 0.4240, 0.0460, 0.0800]]), 'area': tensor([8522.2217, 1916.6666]), 'iscrowd': tensor([0, 0]), 'orig_size': tensor([480, 480])}}
```

æˆ‘ä»¬å¿«å®Œæˆäº† ğŸš€ã€‚ä½œä¸ºæœ€åçš„é¢„å¤„ç†æ­¥éª¤ï¼Œæˆ‘ä»¬éœ€è¦ç¼–å†™ä¸€ä¸ªè‡ªå®šä¹‰çš„ `collate_fn`ã€‚é‚£ä¹ˆï¼Œä»€ä¹ˆæ˜¯ `collate_fn`ï¼Ÿ

`collate_fn` è´Ÿè´£å°†æ•°æ®é›†ä¸­çš„ä¸€ç»„æ ·æœ¬è½¬æ¢ä¸ºé€‚åˆæ¨¡å‹è¾“å…¥æ ¼å¼çš„æ‰¹æ¬¡ã€‚

ä¸€èˆ¬æ¥è¯´ï¼Œ`DataCollator` é€šå¸¸æ‰§è¡Œå¡«å……ã€æˆªæ–­ç­‰ä»»åŠ¡ã€‚åœ¨è‡ªå®šä¹‰ collate å‡½æ•°ä¸­ï¼Œæˆ‘ä»¬é€šå¸¸å®šä¹‰å¦‚ä½•å°†æ•°æ®åˆ†ç»„ä¸ºæ‰¹æ¬¡ï¼Œæˆ–ç®€å•åœ°è¯´ï¼Œå¦‚ä½•è¡¨ç¤ºæ¯ä¸ªæ‰¹æ¬¡ã€‚

æ•°æ®æ•´åˆå™¨ä¸»è¦æ˜¯å°†æ•°æ®ç»„åˆåœ¨ä¸€èµ·å¹¶è¿›è¡Œé¢„å¤„ç†ã€‚è®©æˆ‘ä»¬æ¥ç¼–å†™æˆ‘ä»¬çš„ collate å‡½æ•°ã€‚

```python
def collate_fn(batch):
    pixel_values = [item["pixel_values"] for item in batch]
    encoding = image_processor.pad(pixel_values, return_tensors="pt")
    labels = [item["labels"] for item in batch]
    batch = {}
    batch["pixel_values"] = encoding["pixel_values"]
    batch["pixel_mask"] = encoding["pixel_mask"]
    batch["labels"] = labels
    return batch
```

## è®­ç»ƒ DETR æ¨¡å‹

ç›®å‰ï¼Œæ‰€æœ‰ç¹é‡çš„å·¥ä½œå·²ç»å®Œæˆäº†ã€‚ç°åœ¨ï¼Œå‰©ä¸‹çš„å°±æ˜¯ä¸€æ­¥ä¸€æ­¥ç»„è£…æ¯ä¸ªéƒ¨åˆ†ã€‚è®©æˆ‘ä»¬å¼€å§‹å§ï¼

è®­ç»ƒè¿‡ç¨‹åŒ…å«ä»¥ä¸‹æ­¥éª¤ï¼š

1. ä½¿ç”¨ä¸é¢„å¤„ç†ç›¸åŒçš„æ£€æŸ¥ç‚¹ï¼Œé€šè¿‡ [AutoModelForObjectDetection](https://huggingface.co/docs/transformers/v4.36.0/en/model_doc/auto#transformers.AutoModelForObjectDetection) åŠ è½½åŸºç¡€ï¼ˆé¢„è®­ç»ƒï¼‰æ¨¡å‹ã€‚

2. åœ¨ [TrainingArguments](https://huggingface.co/docs/transformers/v4.36.0/en/main_classes/trainer#transformers.TrainingArguments) ä¸­å®šä¹‰æ‰€æœ‰è¶…å‚æ•°å’Œé™„åŠ å‚æ•°ã€‚

3. å°†è®­ç»ƒå‚æ•°ä¸æ¨¡å‹ã€æ•°æ®é›†å’Œå›¾åƒä¸€èµ·ä¼ é€’åˆ° [HuggingFace Trainer](https://huggingface.co/docs/transformers/v4.36.0/en/main_classes/trainer#transformers.Trainer)ã€‚

4. è°ƒç”¨ `train()` æ–¹æ³•å¾®è°ƒæ¨¡å‹ã€‚

> åœ¨åŠ è½½ä¸é¢„å¤„ç†ç›¸åŒçš„æ£€æŸ¥ç‚¹æ—¶ï¼Œè¯·è®°å¾—ä¼ å…¥æ‚¨ä¹‹å‰ä»æ•°æ®é›†å…ƒæ•°æ®åˆ›å»ºçš„ `label2id` å’Œ `id2label` æ˜ å°„ã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬æŒ‡å®š `ignore_mismatched_sizes=True`ï¼Œä»¥æ›¿æ¢ç°æœ‰çš„åˆ†ç±»å¤´éƒ¨ä¸ºæ–°çš„åˆ†ç±»å¤´éƒ¨ã€‚

```python
from transformers import AutoModelForObjectDetection

id2label = {0: "head", 1: "helmet", 2: "person"}
label2id = {v: k for k, v in id2label.items()}


model = AutoModelForObjectDetection.from_pretrained(
    checkpoint,
    id2label=id2label,
    label2id=label2id,
    ignore_mismatched_sizes=True,
)
```

åœ¨ç»§ç»­ä¹‹å‰ï¼Œç™»å½• Hugging Face Hub ä»¥ä¾¿åœ¨è®­ç»ƒæ—¶å®æ—¶ä¸Šä¼ æ¨¡å‹ã€‚è¿™æ ·ï¼Œæ‚¨æ— éœ€å¤„ç†æ£€æŸ¥ç‚¹å¹¶å°†å…¶å­˜å‚¨åœ¨æŸå¤„ã€‚

```python
from huggingface_hub import notebook_login

notebook_login()
```

å®Œæˆåï¼Œå¼€å§‹è®­ç»ƒæ¨¡å‹ã€‚é¦–å…ˆå®šä¹‰è®­ç»ƒå‚æ•°ï¼Œç„¶åå®šä¹‰ä½¿ç”¨è¿™äº›å‚æ•°è¿›è¡Œè®­ç»ƒçš„ trainer å¯¹è±¡ï¼Œå¦‚ä¸‹æ‰€ç¤ºï¼š

```python
from transformers import TrainingArguments
from transformers import Trainer

# å®šä¹‰è®­ç»ƒå‚æ•°

training_args = TrainingArguments(
    output_dir="detr-resnet-50-hardhat-finetuned",
    per_device_train_batch_size=8,
    num_train_epochs=3,
    max_steps=1000,
    fp16=True,
    save_steps=10,
    logging_steps=30,
    learning_rate=1e-5,
    weight_decay=1e-4,
    save_total_limit=2,
    remove_unused_columns=False,
    push_to_hub=True,
)

# å®šä¹‰ trainer

trainer = Trainer(
    model=model,
    args=training_args,
    data_collator=collate_fn,
    train_dataset=train_dataset_transformed,
    eval_dataset=test_dataset_transformed,
    tokenizer=image_processor,
)

trainer.train()
```

è®­ç»ƒå®Œæˆåï¼Œæ‚¨å¯ä»¥åˆ é™¤æ¨¡å‹ï¼Œå› ä¸ºæ£€æŸ¥ç‚¹å·²ä¸Šä¼ åˆ° Hugging Face Hubã€‚

```python
del model
torch.cuda.synchronize()
```

### æµ‹è¯•å’Œæ¨ç†

ç°åœ¨æˆ‘ä»¬å°†å°è¯•å¯¹æ–°å¾®è°ƒçš„æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚åœ¨æœ¬æ•™ç¨‹ä¸­ï¼Œæˆ‘ä»¬å°†é’ˆå¯¹ä»¥ä¸‹å›¾åƒè¿›è¡Œæµ‹è¯•ï¼š

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/test_input_for_od.png" alt="input-test-image">
<p>å›¾3ï¼šinput-test-image</p>
</div>

æˆ‘ä»¬é¦–å…ˆç¼–å†™ä¸€ä¸ªéå¸¸ç®€å•çš„ä»£ç æ¥è¿›è¡Œæ–°å›¾åƒçš„ç›®æ ‡æ£€æµ‹æ¨ç†ã€‚æˆ‘ä»¬ä»å•å¼ å›¾ç‰‡çš„æ¨ç†å¼€å§‹ï¼Œéšåæˆ‘ä»¬å°†æ•´åˆä¸€åˆ‡å¹¶å°†å…¶åˆ¶ä½œä¸ºä¸€ä¸ªå‡½æ•°ã€‚

```python
import requests
from transformers import pipeline

# ä¸‹è½½æ ·æœ¬å›¾åƒ

url = "https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/test-helmet-object-detection.jpg"
image = Image.open(requests.get(url, stream=True).raw)

# åˆ›å»ºç›®æ ‡æ£€æµ‹ pipeline

obj_detector = pipeline(
    "object-detection", model="anindya64/detr-resnet-50-dc5-hardhat-finetuned"
)
results = obj_detector(train_dataset[0]["image"])

print(results)
```

ç°åœ¨è®©æˆ‘ä»¬ç¼–å†™ä¸€ä¸ªéå¸¸ç®€å•çš„å‡½æ•°æ¥å°†ç»“æœç»˜åˆ¶åœ¨å›¾åƒä¸Šã€‚æˆ‘ä»¬ä»ç»“æœä¸­è·å¾—åˆ†æ•°ã€æ ‡ç­¾å’Œç›¸åº”çš„è¾¹ç•Œæ¡†åæ ‡ï¼Œè¿™äº›å°†ç”¨äºåœ¨å›¾åƒä¸­ç»˜åˆ¶ã€‚

```python
def plot_results(image, results, threshold=0.7):
    image = Image.fromarray(np.uint8(image))
    draw = ImageDraw.Draw(image)
    for result in results:
        score = result["score"]
        label = result["label"]
        box = list(result["box"].values())
        if score > threshold:
            x, y, x2, y2 = tuple(box)
            draw.rectangle((x, y, x2, y2), outline="red", width=1)
            draw.text((x, y), label, fill="white")
            draw.text(
                (x + 0.5, y - 0.5),
                text=str(score),
                fill="green" if score > 0.7 else "red",
            )
    return image
```

æœ€åï¼Œä½¿ç”¨è¯¥å‡½æ•°å¤„ç†æˆ‘ä»¬ä½¿ç”¨çš„ç›¸åŒæµ‹è¯•å›¾åƒã€‚

```
results = obj_detector(image)
plot_results(image, results)
```

è¾“å‡ºå°†å¦‚ä¸‹æ‰€ç¤ºï¼š

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/test_output_for_od.png" alt="output-test-image-plot">
<p>å›¾4ï¼šoutput-test-image-plot</p>
</div>

ç°åœ¨ï¼Œè®©æˆ‘ä»¬å°†æ‰€æœ‰å†…å®¹ç»„åˆæˆä¸€ä¸ªç®€å•çš„å‡½æ•°ã€‚

```python
def predict(image, pipeline, threshold=0.7):
    results = pipeline(image)
    return plot_results(image, results, threshold)


# è®©æˆ‘ä»¬æµ‹è¯•å¦ä¸€ä¸ªæµ‹è¯•å›¾åƒ

img = test_dataset[0]["image"]
predict(img, obj_detector)
```

è®©æˆ‘ä»¬ä½¿ç”¨æˆ‘ä»¬çš„æ¨ç†å‡½æ•°å¯¹å¤šä¸ªå›¾åƒè¿›è¡Œç»˜åˆ¶ã€‚

```python 
from tqdm.auto import tqdm


def plot_images(dataset, indices):
    """
    ç»˜åˆ¶å›¾åƒåŠå…¶æ³¨é‡Šã€‚
    """
    num_rows = len(indices) // 3
    num_cols = 3
    fig, axes = plt.subplots(num_rows, num_cols, figsize=(15, 10))

    for i, idx in tqdm(enumerate(indices), total=len(indices)):
        row = i // num_cols
        col = i % num_cols

        # ç»˜åˆ¶å›¾åƒ
        image = predict(dataset[idx]["image"], obj_detector)

        # åœ¨ç›¸åº”çš„å­å›¾ä¸Šæ˜¾ç¤ºå›¾åƒ
        axes[row, col].imshow(image)
        axes[row, col].axis("off")

    plt.tight_layout()
    plt.show()


plot_images(test_dataset, range(6))
```
è¿è¡Œæ­¤å‡½æ•°å°†ä¼šç»™å‡ºå¦‚ä¸‹è¾“å‡ºï¼š

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/sample_od_test_set_inference_output.png" alt="test-sample-output-plot">
<p>å›¾5ï¼štest-sample-output-plot</p>
</div>

è¿™ç»“æœè¿˜ä¸é”™ã€‚å¦‚æœæˆ‘ä»¬è¿›ä¸€æ­¥å¾®è°ƒï¼Œè¿˜å¯ä»¥æ”¹å–„ç»“æœã€‚æ‚¨å¯ä»¥åœ¨æ­¤æ‰¾åˆ°è¯¥å¾®è°ƒæ£€æŸ¥ç‚¹ [here](hf-vision/detr-resnet-50-dc5-harhat-finetuned)ã€‚