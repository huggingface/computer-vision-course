# åº¦é‡å’Œç›¸å¯¹å•ç›®æ·±åº¦ä¼°è®¡ï¼šæ¦‚è¿°ã€‚å¾®è°ƒ Depth Anything V2 ğŸ‘ ğŸ“š

## æ¨¡å‹çš„æ¼”å˜

åœ¨è¿‡å»çš„åå¹´ä¸­ï¼Œå•ç›®æ·±åº¦ä¼°è®¡æ¨¡å‹å–å¾—äº†æ˜¾è‘—çš„è¿›æ­¥ã€‚è®©æˆ‘ä»¬é€šè¿‡è§†è§‰ä¹‹æ—…æ¥å›é¡¾è¿™ä¸€æ¼”å˜ã€‚

æˆ‘ä»¬ä»è¿™æ ·çš„åŸºæœ¬æ¨¡å‹å¼€å§‹ï¼š

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/Metric%20and%20Relative%20Monocular%20Depth%20Estimation%20An%20Overview.%20Fine-Tuning%20Depth%20Anything%20V2/depth_estimation_evolution1.png" alt="image/png">
<p>å›¾1ï¼šimage/png</p>
</div>
å‘å±•åˆ°æ›´å¤æ‚çš„æ¨¡å‹ï¼š

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/Metric%20and%20Relative%20Monocular%20Depth%20Estimation%20An%20Overview.%20Fine-Tuning%20Depth%20Anything%20V2/depth_estimation_evolution2.png" alt="image/png">
<p>å›¾2ï¼šimage/png</p>
</div>

è€Œç°åœ¨ï¼Œæˆ‘ä»¬æœ‰äº†æœ€å…ˆè¿›çš„æ¨¡å‹ï¼ŒDepth Anything V2ï¼š

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/Metric%20and%20Relative%20Monocular%20Depth%20Estimation%20An%20Overview.%20Fine-Tuning%20Depth%20Anything%20V2/depth_estimation_evolution3.png" alt="image/png">
<p>å›¾3ï¼šimage/png</p>
</div>

å¾ˆä»¤äººæƒŠå¹ï¼Œä¸æ˜¯å—ï¼Ÿ

ä»Šå¤©ï¼Œæˆ‘ä»¬å°†æ­å¼€è¿™äº›æ¨¡å‹çš„å·¥ä½œåŸç†ï¼Œç®€åŒ–å¤æ‚çš„æ¦‚å¿µã€‚æ­¤å¤–ï¼Œæˆ‘ä»¬å°†ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†å¾®è°ƒæˆ‘ä»¬è‡ªå·±çš„æ¨¡å‹ã€‚â€œä½†æ˜¯ç­‰ç­‰ï¼Œâ€ä½ å¯èƒ½ä¼šé—®ï¼Œâ€œå½“æœ€æ–°çš„æ¨¡å‹åœ¨ä»»ä½•ç¯å¢ƒä¸­éƒ½è¡¨ç°å¾—å¦‚æ­¤å‡ºè‰²æ—¶ï¼Œä¸ºä»€ä¹ˆæˆ‘ä»¬éœ€è¦åœ¨è‡ªå·±çš„æ•°æ®é›†ä¸Šå¾®è°ƒæ¨¡å‹å‘¢ï¼Ÿâ€

è¿™å°±æ˜¯ç»†å¾®å·®åˆ«å’Œå…·ä½“æƒ…å†µå‘æŒ¥ä½œç”¨çš„åœ°æ–¹ï¼Œè€Œè¿™æ­£æ˜¯æœ¬æ–‡çš„é‡ç‚¹ã€‚å¦‚æœä½ æ¸´æœ›æ¢ç´¢å•ç›®æ·±åº¦ä¼°è®¡çš„å¤æ‚æ€§ï¼Œè¯·ç»§ç»­é˜…è¯»ã€‚

## åŸºç¡€

â€œå¥½çš„ï¼Œæ·±åº¦åˆ°åº•æ˜¯ä»€ä¹ˆï¼Ÿâ€é€šå¸¸ï¼Œå®ƒæ˜¯ä¸€ä¸ªå•é€šé“å›¾åƒï¼Œå…¶ä¸­æ¯ä¸ªåƒç´ ä»£è¡¨ä»ç›¸æœºæˆ–ä¼ æ„Ÿå™¨åˆ°ä¸è¯¥åƒç´ å¯¹åº”çš„ç©ºé—´ç‚¹çš„è·ç¦»ã€‚ç„¶è€Œï¼Œäº‹å®è¯æ˜ï¼Œè¿™äº›è·ç¦»å¯ä»¥æ˜¯ç»å¯¹çš„æˆ–ç›¸å¯¹çš„â€”â€”çœŸæ˜¯ä¸ªè½¬æŠ˜ï¼
- **ç»å¯¹æ·±åº¦**ï¼šæ¯ä¸ªåƒç´ å€¼ç›´æ¥å¯¹åº”ä¸€ä¸ªç‰©ç†è·ç¦»ï¼ˆä¾‹å¦‚ï¼Œä»¥ç±³æˆ–å˜ç±³ä¸ºå•ä½ï¼‰ã€‚
- **ç›¸å¯¹æ·±åº¦**ï¼šåƒç´ å€¼æŒ‡ç¤ºå“ªäº›ç‚¹æ›´è¿‘æˆ–æ›´è¿œï¼Œè€Œä¸å‚è€ƒç°å®ä¸–ç•Œçš„æµ‹é‡å•ä½ã€‚ç›¸å¯¹æ·±åº¦é€šå¸¸æ˜¯åè½¬çš„ï¼Œå³æ•°å­—è¶Šå°ï¼Œç‚¹è¶Šè¿œã€‚

æˆ‘ä»¬ç¨åå°†æ›´è¯¦ç»†åœ°æ¢è®¨è¿™äº›æ¦‚å¿µã€‚

â€œé‚£ä¹ˆï¼Œå•ç›®æ˜¯ä»€ä¹ˆæ„æ€ï¼Ÿâ€å®ƒåªæ˜¯æ„å‘³ç€æˆ‘ä»¬éœ€è¦ä»…ä½¿ç”¨ä¸€å¼ ç…§ç‰‡æ¥ä¼°è®¡æ·±åº¦ã€‚è¿™æœ‰ä»€ä¹ˆæŒ‘æˆ˜å‘¢ï¼Ÿçœ‹çœ‹è¿™ä¸ªï¼š

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/Metric%20and%20Relative%20Monocular%20Depth%20Estimation%20An%20Overview.%20Fine-Tuning%20Depth%20Anything%20V2/depth_ambiguity1.gif" alt="image/gif">
<p>å›¾4ï¼šimage/gif</p>
</div>

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/Metric%20and%20Relative%20Monocular%20Depth%20Estimation%20An%20Overview.%20Fine-Tuning%20Depth%20Anything%20V2/depth_ambiguity2.gif" alt="image/gif">
<p>å›¾5ï¼šimage/gif</p>
</div>

å¦‚ä½ æ‰€è§ï¼Œç”±äºé€è§†ï¼Œå°† 3D ç©ºé—´æŠ•å½±åˆ° 2D å¹³é¢ä¸Šä¼šäº§ç”Ÿæ­§ä¹‰ã€‚ä¸ºäº†è§£å†³è¿™ä¸ªé—®é¢˜ï¼Œæœ‰ä½¿ç”¨å¤šå¼ å›¾åƒè¿›è¡Œæ·±åº¦ä¼°è®¡çš„ç²¾ç¡®æ•°å­¦æ–¹æ³•ï¼Œä¾‹å¦‚ç«‹ä½“è§†è§‰ã€è¿åŠ¨ç»“æ„å’Œæ›´å¹¿æ³›çš„æ‘„å½±æµ‹é‡é¢†åŸŸã€‚æ­¤å¤–ï¼Œå¯ä»¥ä½¿ç”¨æ¿€å…‰æ‰«æä»ªï¼ˆä¾‹å¦‚ LiDARï¼‰ç­‰æŠ€æœ¯è¿›è¡Œæ·±åº¦æµ‹é‡ã€‚

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/Metric%20and%20Relative%20Monocular%20Depth%20Estimation%20An%20Overview.%20Fine-Tuning%20Depth%20Anything%20V2/stereo_vision_sfm.png" alt="image/png">
<p>å›¾6ï¼šimage/png</p>
</div>

## ç›¸å¯¹æ·±åº¦å’Œç»å¯¹æ·±åº¦ï¼ˆåˆååº¦é‡æ·±åº¦ï¼‰ä¼°è®¡ï¼šæœ‰ä»€ä¹ˆæ„ä¹‰ï¼Ÿ

è®©æˆ‘ä»¬æ¢è®¨ä¸€äº›çªå‡ºç›¸å¯¹æ·±åº¦ä¼°è®¡å¿…è¦æ€§çš„æŒ‘æˆ˜ã€‚ä¸ºäº†æ›´ç§‘å­¦ï¼Œè®©æˆ‘ä»¬å‚è€ƒä¸€äº›è®ºæ–‡ã€‚

>é¢„æµ‹åº¦é‡æ·±åº¦çš„ä¼˜åŠ¿åœ¨äºå®ƒåœ¨è®¡ç®—æœºè§†è§‰å’Œæœºå™¨äººæŠ€æœ¯ä¸­çš„è®¸å¤šä¸‹æ¸¸åº”ç”¨ä¸­å…·æœ‰å®é™…ç”¨é€”ï¼Œä¾‹å¦‚æ˜ å°„ã€è§„åˆ’ã€å¯¼èˆªã€ç‰©ä½“è¯†åˆ«ã€3D é‡å»ºå’Œå›¾åƒç¼–è¾‘ã€‚ç„¶è€Œï¼Œåœ¨å¤šä¸ªæ•°æ®é›†ä¸Šè®­ç»ƒå•ä¸ªåº¦é‡æ·±åº¦ä¼°è®¡æ¨¡å‹é€šå¸¸ä¼šé™ä½æ€§èƒ½ï¼Œç‰¹åˆ«æ˜¯å½“æ”¶é›†çš„å›¾åƒåœ¨æ·±åº¦å°ºåº¦ä¸Šæœ‰å¾ˆå¤§å·®å¼‚æ—¶ï¼Œä¾‹å¦‚å®¤å†…å’Œå®¤å¤–å›¾åƒã€‚å› æ­¤ï¼Œå½“å‰çš„ MDE æ¨¡å‹é€šå¸¸è¿‡åº¦æ‹Ÿåˆç‰¹å®šæ•°æ®é›†ï¼Œå¹¶ä¸”ä¸èƒ½å¾ˆå¥½åœ°æ¨å¹¿åˆ°å…¶ä»–æ•°æ®é›†ã€‚

é€šå¸¸ï¼Œè¿™ç§å›¾åƒåˆ°å›¾åƒä»»åŠ¡çš„æ¶æ„æ˜¯ä¸€ä¸ªç¼–ç å™¨ - è§£ç å™¨æ¨¡å‹ï¼Œå¦‚ U-Netï¼Œå¹¶è¿›è¡Œäº†å„ç§ä¿®æ”¹ã€‚å½¢å¼ä¸Šï¼Œè¿™æ˜¯ä¸€ä¸ªé€åƒç´ å›å½’é—®é¢˜ã€‚æƒ³è±¡ä¸€ä¸‹ï¼Œå¯¹äºç¥ç»ç½‘ç»œæ¥è¯´ï¼Œå‡†ç¡®é¢„æµ‹æ¯ä¸ªåƒç´ çš„è·ç¦»æ˜¯å¤šä¹ˆå…·æœ‰æŒ‘æˆ˜æ€§ï¼Œè·ç¦»èŒƒå›´ä»å‡ ç±³åˆ°å‡ ç™¾ç±³ã€‚<br>è¿™è®©æˆ‘ä»¬æƒ³åˆ°æ”¾å¼ƒåœ¨æ‰€æœ‰åœºæ™¯ä¸­é¢„æµ‹ç²¾ç¡®è·ç¦»çš„é€šç”¨æ¨¡å‹ã€‚ç›¸åï¼Œè®©æˆ‘ä»¬å¼€å‘ä¸€ä¸ªè¿‘ä¼¼ï¼ˆç›¸å¯¹ï¼‰é¢„æµ‹æ·±åº¦çš„æ¨¡å‹ï¼Œé€šè¿‡æŒ‡ç¤ºå“ªäº›ç‰©ä½“ç›¸å¯¹äºå½¼æ­¤å’Œæˆ‘ä»¬æ›´è¿œï¼Œå“ªäº›æ›´è¿‘ï¼Œæ¥æ•æ‰åœºæ™¯çš„å½¢çŠ¶å’Œç»“æ„ã€‚å¦‚æœéœ€è¦ç²¾ç¡®è·ç¦»ï¼Œæˆ‘ä»¬å¯ä»¥åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šå¾®è°ƒè¿™ä¸ªç›¸å¯¹æ¨¡å‹ï¼Œåˆ©ç”¨å®ƒå¯¹ä»»åŠ¡çš„ç°æœ‰ç†è§£ã€‚

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/Metric%20and%20Relative%20Monocular%20Depth%20Estimation%20An%20Overview.%20Fine-Tuning%20Depth%20Anything%20V2/different_scales.png" alt="image/png">
<p>å›¾7ï¼šimage/png</p>
</div>

è¿˜æœ‰æ›´å¤šç»†èŠ‚æˆ‘ä»¬éœ€è¦æ³¨æ„ã€‚

>æ¨¡å‹ä¸ä»…å¿…é¡»å¤„ç†ä½¿ç”¨ä¸åŒç›¸æœºå’Œç›¸æœºè®¾ç½®æ‹æ‘„çš„å›¾åƒï¼Œè¿˜å¿…é¡»å­¦ä¼šè°ƒæ•´åœºæ™¯æ•´ä½“å°ºåº¦çš„å·¨å¤§å˜åŒ–ã€‚

é™¤äº†ä¸åŒçš„å°ºåº¦ï¼Œå¦‚æˆ‘ä»¬å‰é¢æåˆ°çš„ï¼Œä¸€ä¸ªé‡å¤§é—®é¢˜åœ¨äºç›¸æœºæœ¬èº«ï¼Œå®ƒä»¬å¯ä»¥å¯¹ä¸–ç•Œæœ‰æˆªç„¶ä¸åŒçš„è§†è§’ã€‚

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/Metric%20and%20Relative%20Monocular%20Depth%20Estimation%20An%20Overview.%20Fine-Tuning%20Depth%20Anything%20V2/focal_lenght.png" alt="image/png">
<p>å›¾8ï¼šimage/png</p>
</div>

æ³¨æ„ç„¦è·çš„å˜åŒ–å¦‚ä½•æå¤§åœ°æ”¹å˜å¯¹èƒŒæ™¯è·ç¦»çš„æ„ŸçŸ¥ï¼

æœ€åï¼Œè®¸å¤šæ•°æ®é›†å®Œå…¨æ²¡æœ‰ç»å¯¹æ·±åº¦å›¾ï¼Œåªæœ‰ç›¸å¯¹æ·±åº¦å›¾ï¼ˆä¾‹å¦‚ï¼Œç”±äºç¼ºä¹ç›¸æœºæ ¡å‡†ï¼‰ã€‚æ­¤å¤–ï¼Œæ¯ç§è·å–æ·±åº¦çš„æ–¹æ³•éƒ½æœ‰å…¶è‡ªèº«çš„ä¼˜ç‚¹ã€ç¼ºç‚¹ã€åå·®å’Œé—®é¢˜ã€‚

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/Metric%20and%20Relative%20Monocular%20Depth%20Estimation%20An%20Overview.%20Fine-Tuning%20Depth%20Anything%20V2/metrics1.png" alt="image/png">
<p>å›¾9ï¼šimage/png</p>
</div>

>æˆ‘ä»¬ç¡®å®šäº†ä¸‰ä¸ªä¸»è¦æŒ‘æˆ˜ã€‚1ï¼‰æ·±åº¦çš„å›ºæœ‰ä¸åŒè¡¨ç¤ºï¼šç›´æ¥æ·±åº¦è¡¨ç¤ºä¸åå‘æ·±åº¦è¡¨ç¤ºã€‚2ï¼‰å°ºåº¦æ¨¡ç³Šæ€§ï¼šå¯¹äºæŸäº›æ•°æ®æºï¼Œæ·±åº¦ä»…åœ¨æœªçŸ¥å°ºåº¦ä¸‹ç»™å‡ºã€‚3ï¼‰ä½ç§»æ¨¡ç³Šæ€§ï¼šä¸€äº›æ•°æ®é›†ä»…åœ¨æœªçŸ¥å°ºåº¦å’Œå…¨å±€è§†å·®ä½ç§»ä¸‹æä¾›è§†å·®ï¼Œå…¨å±€è§†å·®ä½ç§»æ˜¯æœªçŸ¥åŸºçº¿å’Œç”±äºåå¤„ç†å¼•èµ·çš„ä¸»ç‚¹æ°´å¹³ä½ç§»çš„å‡½æ•°ã€‚

*è§†å·®æ˜¯æŒ‡ä»ä¸¤ä¸ªä¸åŒè§†ç‚¹è§‚å¯Ÿç‰©ä½“æ—¶ç‰©ä½“çš„æ˜æ˜¾ä½ç½®å·®å¼‚ï¼Œå¸¸ç”¨äºç«‹ä½“è§†è§‰ä¸­ä¼°è®¡æ·±åº¦ã€‚*

ç®€è€Œè¨€ä¹‹ï¼Œæˆ‘å¸Œæœ›æˆ‘å·²ç»è¯´æœä½ ï¼Œä½ ä¸èƒ½åªæ˜¯ä»äº’è”ç½‘ä¸Šè·å–åˆ†æ•£çš„æ·±åº¦å›¾ï¼Œå¹¶ä½¿ç”¨ä¸€äº›é€åƒç´ çš„å‡æ–¹è¯¯å·®æ¥è®­ç»ƒæ¨¡å‹ã€‚

ä½†æ˜¯æˆ‘ä»¬å¦‚ä½•å¹³è¡¡æ‰€æœ‰è¿™äº›å˜åŒ–å‘¢ï¼Ÿæˆ‘ä»¬å¦‚ä½•å°½å¯èƒ½åœ°ä»å·®å¼‚ä¸­æŠ½è±¡å‡ºæ¥ï¼Œå¹¶ä»æ‰€æœ‰è¿™äº›æ•°æ®é›†ä¸­æå–å…±æ€§â€”â€”å³åœºæ™¯çš„å½¢çŠ¶å’Œç»“æ„ã€ç‰©ä½“ä¹‹é—´çš„æ¯”ä¾‹å…³ç³»ï¼ŒæŒ‡ç¤ºä»€ä¹ˆæ›´è¿‘ï¼Œä»€ä¹ˆæ›´è¿œï¼Ÿ

## å°ºåº¦å’Œä½ç§»ä¸å˜æŸå¤±ğŸ˜

ç®€è€Œè¨€ä¹‹ï¼Œæˆ‘ä»¬éœ€è¦å¯¹è¦è¿›è¡Œè®­ç»ƒçš„æ‰€æœ‰æ·±åº¦å›¾è¿›è¡ŒæŸç§å½’ä¸€åŒ–å¤„ç†ï¼Œå¹¶ä½¿ç”¨è¯¥å½’ä¸€åŒ–åçš„æ·±åº¦å›¾è¯„ä¼°æŒ‡æ ‡ã€‚è¿™é‡Œæœ‰ä¸€ä¸ªæƒ³æ³•ï¼šæˆ‘ä»¬å¸Œæœ›åˆ›å»ºä¸€ä¸ªä¸è€ƒè™‘ç¯å¢ƒå°ºåº¦æˆ–å„ç§ä½ç§»çš„æŸå¤±å‡½æ•°ã€‚å‰©ä¸‹çš„ä»»åŠ¡æ˜¯å°†è¿™ä¸ªæƒ³æ³•è½¬åŒ–ä¸ºæ•°å­¦æœ¯è¯­ã€‚

>å…·ä½“æ¥è¯´ï¼Œé¦–å…ˆé€šè¿‡$d=\frac{1}{t}$å°†æ·±åº¦å€¼è½¬æ¢åˆ°è§†å·®ç©ºé—´ï¼Œç„¶ååœ¨æ¯ä¸ªæ·±åº¦å›¾ä¸Šå°†å…¶å½’ä¸€åŒ–åˆ°$0\sim1$ã€‚ä¸ºäº†å®ç°å¤šæ•°æ®é›†è”åˆè®­ç»ƒï¼Œæˆ‘ä»¬é‡‡ç”¨ä»¿å°„ä¸å˜æŸå¤±æ¥å¿½ç•¥æ¯ä¸ªæ ·æœ¬çš„æœªçŸ¥å°ºåº¦å’Œä½ç§»ï¼š
$$\mathcal{L}_1=\frac{1}{HW}\sum_{i=1}^{HW}\rho(d_i^*,d_i),$$
å…¶ä¸­$d_i^*$å’Œ$d_i$åˆ†åˆ«æ˜¯é¢„æµ‹å€¼å’ŒçœŸå®å€¼ã€‚è€Œ$\rho$æ˜¯ä»¿å°„ä¸å˜å¹³å‡ç»å¯¹è¯¯å·®æŸå¤±ï¼š$\rho(d_i^*,d_i)=|\hat{d}_i^*-\hat{d}_i|$ï¼Œå…¶ä¸­$\hat{d}_i^*$å’Œ$\hat{d}_i$æ˜¯ç»è¿‡ç¼©æ”¾å’Œä½ç§»åçš„é¢„æµ‹å€¼$d_i^*$å’ŒçœŸå®å€¼$d_i$ï¼š
$$\hat{d}_i=\frac{d_i-t(d)}{s(d)},$$
å…¶ä¸­$t(d)$å’Œ$s(d)$ç”¨äºä½¿é¢„æµ‹å€¼å’ŒçœŸå®å€¼å…·æœ‰é›¶å¹³ç§»å’Œå•ä½å°ºåº¦ï¼š
$$t(d)=\mathrm{median}(d),\quad s(d)=\frac{1}{HW}\sum_{i=1}^{HW}|d_i-t(d)|.$$

å®é™…ä¸Šï¼Œè¿˜æœ‰è®¸å¤šå…¶ä»–æ–¹æ³•å’Œå‡½æ•°æœ‰åŠ©äºæ¶ˆé™¤å°ºåº¦å’Œä½ç§»ã€‚æŸå¤±å‡½æ•°ä¹Ÿæœ‰ä¸åŒçš„æ·»åŠ é¡¹ï¼Œä¾‹å¦‚æ¢¯åº¦æŸå¤±ï¼Œå®ƒä¸å…³æ³¨åƒç´ å€¼æœ¬èº«ï¼Œè€Œæ˜¯å…³æ³¨å®ƒä»¬å˜åŒ–çš„é€Ÿåº¦ï¼ˆå› æ­¤å¾—åâ€”â€”æ¢¯åº¦ï¼‰ã€‚ä½ å¯ä»¥åœ¨[MiDaS](https://arxiv.org/pdf/1907.01341)è®ºæ–‡ä¸­äº†è§£æ›´å¤šç›¸å…³å†…å®¹ï¼Œæˆ‘å°†åœ¨æœ€ååˆ—å‡ºä¸€äº›æœ‰ç”¨çš„æ–‡çŒ®ã€‚åœ¨è¿›å…¥æœ€æ¿€åŠ¨äººå¿ƒçš„éƒ¨åˆ†â€”â€”ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†å¯¹ç»å¯¹æ·±åº¦è¿›è¡Œå¾®è°ƒä¹‹å‰ï¼Œè®©æˆ‘ä»¬å…ˆç®€è¦è®¨è®ºä¸€ä¸‹æŒ‡æ ‡ã€‚

## æŒ‡æ ‡

åœ¨æ·±åº¦ä¼°è®¡ä¸­ï¼Œæœ‰å‡ ä¸ªæ ‡å‡†æŒ‡æ ‡ç”¨äºè¯„ä¼°æ€§èƒ½ï¼ŒåŒ…æ‹¬å¹³å‡ç»å¯¹è¯¯å·®ï¼ˆMAEï¼‰ã€å‡æ–¹æ ¹è¯¯å·®ï¼ˆRMSEï¼‰ä»¥åŠå®ƒä»¬çš„å¯¹æ•°å˜åŒ–å½¢å¼ï¼Œä»¥å¹³æ»‘è·ç¦»ä¸­çš„å¤§å·®è·ã€‚æ­¤å¤–ï¼Œè€ƒè™‘ä»¥ä¸‹å†…å®¹ï¼š
- **ç»å¯¹ç›¸å¯¹è¯¯å·®ï¼ˆAbsRelï¼‰**ï¼šè¿™ä¸ªæŒ‡æ ‡ä¸ MAE ç±»ä¼¼ï¼Œä½†ä»¥ç™¾åˆ†æ¯”è¡¨ç¤ºï¼Œæµ‹é‡é¢„æµ‹è·ç¦»ä¸çœŸå®è·ç¦»å¹³å‡ç›¸å·®çš„ç™¾åˆ†æ¯”ã€‚<br>$\text{AbsRel}=\frac{1}{N}\sum_{i=1}^{N}\frac{|d_i-\hat{d}_i|}{d_i}$
- **é˜ˆå€¼å‡†ç¡®ç‡ï¼ˆ$\delta_1$ï¼‰**ï¼šè¿™ä¸ªæŒ‡æ ‡æµ‹é‡é¢„æµ‹åƒç´ ä¸çœŸå®åƒç´ ç›¸å·®ä¸è¶…è¿‡ 25%çš„æ¯”ä¾‹ã€‚<br>$\delta_1=\text{é¢„æµ‹æ·±åº¦ä¸­æ»¡è¶³}\max\left(\frac{d_i}{\hat{d}_i},\frac{\hat{d}_i}{d_i}\right)<1.25\text{çš„æ¯”ä¾‹}$

### é‡è¦è€ƒè™‘å› ç´ 
>å¯¹äºæˆ‘ä»¬æ‰€æœ‰çš„æ¨¡å‹å’ŒåŸºçº¿ï¼Œåœ¨æµ‹é‡è¯¯å·®ä¹‹å‰ï¼Œæˆ‘ä»¬ä¼šå¯¹æ¯å¼ å›¾åƒçš„é¢„æµ‹å€¼å’ŒçœŸå®å€¼è¿›è¡Œå°ºåº¦å’Œä½ç§»å¯¹é½ã€‚

å®é™…ä¸Šï¼Œå¦‚æœæˆ‘ä»¬æ­£åœ¨è®­ç»ƒä»¥é¢„æµ‹ç›¸å¯¹æ·±åº¦ï¼Œä½†æƒ³åœ¨å…·æœ‰ç»å¯¹å€¼çš„æ•°æ®é›†ä¸Šæµ‹é‡è´¨é‡ï¼Œå¹¶ä¸”æˆ‘ä»¬å¯¹åœ¨è¿™ä¸ªæ•°æ®é›†ä¸Šè¿›è¡Œå¾®è°ƒæˆ–å¯¹ç»å¯¹å€¼ä¸æ„Ÿå…´è¶£ï¼Œæˆ‘ä»¬å¯ä»¥åƒæŸå¤±å‡½æ•°ä¸€æ ·ï¼Œä»è®¡ç®—ä¸­æ’é™¤å°ºåº¦å’Œä½ç§»ï¼Œå¹¶å°†æ‰€æœ‰å†…å®¹æ ‡å‡†åŒ–ä¸ºç»Ÿä¸€çš„åº¦é‡ã€‚

### è®¡ç®—æŒ‡æ ‡çš„å››ç§æ–¹æ³•

ç†è§£è¿™äº›æ–¹æ³•æœ‰åŠ©äºåœ¨åˆ†æè®ºæ–‡ä¸­çš„æŒ‡æ ‡æ—¶é¿å…æ··æ·†ï¼š

1. **é›¶æ ·æœ¬ç›¸å¯¹æ·±åº¦ä¼°è®¡**
    - åœ¨ä¸€ç»„æ•°æ®é›†ä¸Šè®­ç»ƒä»¥é¢„æµ‹ç›¸å¯¹æ·±åº¦ï¼Œå¹¶åœ¨å…¶ä»–æ•°æ®é›†ä¸Šæµ‹é‡è´¨é‡ã€‚ç”±äºæ·±åº¦æ˜¯ç›¸å¯¹çš„ï¼Œæ˜¾è‘—ä¸åŒçš„å°ºåº¦ä¸æ˜¯é—®é¢˜ï¼Œå…¶ä»–æ•°æ®é›†ä¸Šçš„æŒ‡æ ‡é€šå¸¸ä»ç„¶å¾ˆé«˜ï¼Œç±»ä¼¼äºè®­ç»ƒæ•°æ®é›†çš„æµ‹è¯•é›†ã€‚

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/Metric%20and%20Relative%20Monocular%20Depth%20Estimation%20An%20Overview.%20Fine-Tuning%20Depth%20Anything%20V2/metrics2.png" alt="image/png">
<p>å›¾10ï¼šimage/png</p>
</div>

2. **é›¶æ ·æœ¬ç»å¯¹æ·±åº¦ä¼°è®¡**
    - è®­ç»ƒä¸€ä¸ªé€šç”¨çš„ç›¸å¯¹æ¨¡å‹ï¼Œç„¶ååœ¨ä¸€ä¸ªå¥½çš„æ•°æ®é›†ä¸Šå¯¹å…¶è¿›è¡Œå¾®è°ƒä»¥é¢„æµ‹ç»å¯¹æ·±åº¦ï¼Œå¹¶åœ¨ä¸åŒçš„æ•°æ®é›†ä¸Šæµ‹é‡ç»å¯¹æ·±åº¦é¢„æµ‹çš„è´¨é‡ã€‚åœ¨è¿™ç§æƒ…å†µä¸‹ï¼ŒæŒ‡æ ‡å¾€å¾€æ¯”å‰ä¸€ç§æ–¹æ³•æ›´å·®ï¼Œçªå‡ºäº†åœ¨ä¸åŒç¯å¢ƒä¸­å¾ˆå¥½åœ°é¢„æµ‹ç»å¯¹æ·±åº¦çš„æŒ‘æˆ˜ã€‚

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/Metric%20and%20Relative%20Monocular%20Depth%20Estimation%20An%20Overview.%20Fine-Tuning%20Depth%20Anything%20V2/metrics3.png" alt="image/png">
<p>å›¾11ï¼šimage/png</p>
</div>

3. **å¾®è°ƒï¼ˆåŸŸå†…ï¼‰ç»å¯¹æ·±åº¦ä¼°è®¡**
    - ä¸å‰ä¸€ç§æ–¹æ³•ç±»ä¼¼ï¼Œä½†ç°åœ¨åœ¨ç”¨äºå¾®è°ƒç»å¯¹æ·±åº¦é¢„æµ‹çš„æ•°æ®é›†çš„æµ‹è¯•é›†ä¸Šæµ‹é‡è´¨é‡ã€‚è¿™æ˜¯æœ€å®ç”¨çš„æ–¹æ³•ä¹‹ä¸€ã€‚

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/Metric%20and%20Relative%20Monocular%20Depth%20Estimation%20An%20Overview.%20Fine-Tuning%20Depth%20Anything%20V2/metrics4.png" alt="image/png">
<p>å›¾12ï¼šimage/png</p>
</div>

4. **å¾®è°ƒï¼ˆåŸŸå†…ï¼‰ç›¸å¯¹æ·±åº¦ä¼°è®¡**
    - è®­ç»ƒä»¥é¢„æµ‹ç›¸å¯¹æ·±åº¦ï¼Œå¹¶åœ¨è®­ç»ƒæ•°æ®é›†çš„æµ‹è¯•é›†ä¸Šæµ‹é‡è´¨é‡ã€‚è¿™å¯èƒ½ä¸æ˜¯æœ€å‡†ç¡®çš„åç§°ï¼Œä½†æƒ³æ³•å¾ˆç®€å•ã€‚

## Depth Anything V2 ç»å¯¹æ·±åº¦ä¼°è®¡å¾®è°ƒ

åœ¨æœ¬èŠ‚ä¸­ï¼Œæˆ‘ä»¬å°†é€šè¿‡åœ¨ NYU-D æ•°æ®é›†ä¸Šå¾®è°ƒæ¨¡å‹ä»¥é¢„æµ‹ç»å¯¹æ·±åº¦æ¥é‡ç° Depth Anything V2 è®ºæ–‡ä¸­çš„ç»“æœï¼Œæ—¨åœ¨å®ç°ä¸ä¸Šä¸€èŠ‚æœ€åä¸€ä¸ªè¡¨æ ¼ä¸­æ‰€ç¤ºçš„æŒ‡æ ‡ç±»ä¼¼çš„æŒ‡æ ‡ã€‚

### Depth Anything V2 èƒŒåçš„å…³é”®æ€æƒ³
Depth Anything V2 æ˜¯ä¸€ä¸ªå¼ºå¤§çš„æ·±åº¦ä¼°è®¡æ¨¡å‹ï¼Œç”±äºå‡ ä¸ªåˆ›æ–°æ¦‚å¿µè€Œå–å¾—äº†æ˜¾è‘—çš„ç»“æœï¼š

- **å¼‚æ„æ•°æ®ä¸Šçš„é€šç”¨è®­ç»ƒæ–¹æ³•**ï¼šè¿™ä¸ªæ–¹æ³•åœ¨ MiDaS 2020 è®ºæ–‡ä¸­å¼•å…¥ï¼Œèƒ½å¤Ÿåœ¨å„ç§ç±»å‹çš„æ•°æ®é›†ä¸Šè¿›è¡Œç¨³å¥çš„è®­ç»ƒã€‚
- **DPT æ¶æ„**ï¼šâ€œç”¨äºå¯†é›†é¢„æµ‹çš„è§†è§‰ Transformerâ€è®ºæ–‡æå‡ºäº†è¿™ç§æ¶æ„ï¼Œå®ƒæœ¬è´¨ä¸Šæ˜¯ä¸€ä¸ªå¸¦æœ‰è§†è§‰ Transformerï¼ˆViTï¼‰ç¼–ç å™¨çš„ U-Net ä»¥åŠä¸€äº›ä¿®æ”¹ã€‚

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/Metric%20and%20Relative%20Monocular%20Depth%20Estimation%20An%20Overview.%20Fine-Tuning%20Depth%20Anything%20V2/dpt.png" alt="image/png">
<p>å›¾13ï¼šimage/png</p>
</div>

- **DINOv2 ç¼–ç å™¨**ï¼šè¿™ä¸ªæ ‡å‡†çš„ ViT é€šè¿‡åœ¨å¤§è§„æ¨¡æ•°æ®é›†ä¸Šä½¿ç”¨è‡ªç›‘ç£æ–¹æ³•è¿›è¡Œé¢„è®­ç»ƒï¼Œä½œä¸ºä¸€ä¸ªå¼ºå¤§è€Œé€šç”¨çš„ç‰¹å¾æå–å™¨ã€‚è¿‘å¹´æ¥ï¼Œè®¡ç®—æœºè§†è§‰ç ”ç©¶äººå‘˜ä¸€ç›´è‡´åŠ›äºåˆ›å»ºç±»ä¼¼äºè‡ªç„¶è¯­è¨€å¤„ç†ä¸­çš„ GPT å’Œ BERT çš„åŸºç¡€æ¨¡å‹ï¼ŒDINOv2 æ˜¯æœç€è¿™ä¸ªæ–¹å‘è¿ˆå‡ºçš„é‡è¦ä¸€æ­¥ã€‚
- **ä½¿ç”¨åˆæˆæ•°æ®**ï¼šè®­ç»ƒæµç¨‹åœ¨ä¸‹é¢çš„å›¾åƒä¸­å¾—åˆ°äº†å¾ˆå¥½çš„æè¿°ã€‚è¿™ç§æ–¹æ³•ä½¿ä½œè€…èƒ½å¤Ÿåœ¨æ·±åº¦å›¾ä¸­å®ç°å¦‚æ­¤æ¸…æ™°å’Œå‡†ç¡®çš„æ•ˆæœã€‚æ¯•ç«Ÿï¼Œå¦‚æœä½ ä»”ç»†æƒ³æƒ³ï¼Œä»åˆæˆæ•°æ®ä¸­è·å¾—çš„æ ‡ç­¾ç¡®å®æ˜¯â€œçœŸå®å€¼â€ã€‚

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/Metric%20and%20Relative%20Monocular%20Depth%20Estimation%20An%20Overview.%20Fine-Tuning%20Depth%20Anything%20V2/DA2_pipeline.png" alt="image/png">
<p>å›¾14ï¼šimage/png</p>
</div>

### å¾®è°ƒå…¥é—¨

ç°åœ¨ï¼Œè®©æˆ‘ä»¬æ·±å…¥ä»£ç ã€‚å¦‚æœä½ æ— æ³•ä½¿ç”¨å¼ºå¤§çš„ GPUï¼Œæˆ‘å¼ºçƒˆæ¨èä½¿ç”¨ Kaggle è€Œä¸æ˜¯ Colabã€‚Kaggle æœ‰å‡ ä¸ªä¼˜ç‚¹ï¼š
- æ¯å‘¨æœ€å¤š 30 å°æ—¶çš„ GPU ä½¿ç”¨æ—¶é—´ã€‚
- æ²¡æœ‰è¿æ¥ä¸­æ–­ã€‚
- éå¸¸å¿«é€Ÿä¸”æ–¹ä¾¿åœ°è®¿é—®æ•°æ®é›†ã€‚
- åœ¨å…¶ä¸­ä¸€ç§é…ç½®ä¸­èƒ½å¤ŸåŒæ—¶ä½¿ç”¨ä¸¤ä¸ª GPUï¼Œè¿™å°†å¸®åŠ©ä½ ç»ƒä¹ åˆ†å¸ƒå¼è®­ç»ƒã€‚

ä½ å¯ä»¥ä½¿ç”¨è¿™ä¸ª[Kaggle ç¬”è®°æœ¬](https://www.kaggle.com/code/amanattheedge/depth-anything-v2-metric-fine-tunning-on-nyu/notebook)ç›´æ¥è¿›å…¥ä»£ç ã€‚

æˆ‘ä»¬å°†åœ¨è¿™é‡Œè¯¦ç»†ä»‹ç»æ‰€æœ‰å†…å®¹ã€‚é¦–å…ˆï¼Œè®©æˆ‘ä»¬ä»ä½œè€…çš„å­˜å‚¨åº“ä¸‹è½½æ‰€æœ‰å¿…è¦çš„æ¨¡å—ä»¥åŠå¸¦æœ‰ ViT-S ç¼–ç å™¨çš„æœ€å°æ¨¡å‹çš„æ£€æŸ¥ç‚¹ã€‚

#### æ­¥éª¤ 1ï¼šå…‹éš†å­˜å‚¨åº“å¹¶ä¸‹è½½é¢„è®­ç»ƒæƒé‡

```bash
!git clone https://github.com/DepthAnything/Depth-Anything-V2
!wget -O depth_anything_v2_vits.pth https://huggingface.co/depth-anything/Depth-Anything-V2-Small/resolve/main/depth_anything_v2_vits.pth?download=true
```
ä½ ä¹Ÿå¯ä»¥[åœ¨è¿™é‡Œ](http://datasets.lids.mit.edu/fastdepth/data/)ä¸‹è½½æ•°æ®é›†ã€‚

#### æ­¥éª¤ 2ï¼šå¯¼å…¥æ‰€éœ€æ¨¡å—

```python
import numpy as np
import matplotlib.pyplot as plt
import os
from tqdm import tqdm
import cv2
import random
import h5py

import sys
sys.path.append('/kaggle/working/Depth-Anything-V2/metric_depth')

from accelerate import Accelerator
from accelerate.utils import set_seed
from accelerate import notebook_launcher
from accelerate import DistributedDataParallelKwargs

import transformers

import torch
import torchvision
from torchvision.transforms import v2
from torchvision.transforms import Compose
import torch.nn.functional as F
import albumentations as A

from depth_anything_v2.dpt import DepthAnythingV2
from util.loss import SiLogLoss
from dataset.transform import Resize, NormalizeImage, PrepareForNet, Crop
```

#### æ­¥éª¤ 3ï¼šè·å–è®­ç»ƒå’ŒéªŒè¯çš„æ‰€æœ‰æ–‡ä»¶è·¯å¾„

```python
def get_all_files(directory):
    all_files = []
    for root, dirs, files in os.walk(directory):
        for file in files:
            all_files.append(os.path.join(root, file))
    return all_files


train_paths = get_all_files('/kaggle/input/nyu-depth-dataset-v2/nyudepthv2/train')
val_paths = get_all_files('/kaggle/input/nyu-depth-dataset-v2/nyudepthv2/val')
```

#### æ­¥éª¤ 4ï¼šå®šä¹‰ PyTorch æ•°æ®é›†

```python
#NYU Depth V2 40k. Original NYU is 400k
class NYU(torch.utils.data.Dataset):
    def __init__(self, paths, mode, size=(518, 518)):
        
        self.mode = mode #train or val
        self.size = size
        self.paths = paths
        
        net_w, net_h = size
        #ä½œè€…çš„å˜æ¢
        self.transform = Compose([
            Resize(
                width=net_w,
                height=net_h,
                resize_target=True if mode == 'train' else False,
                keep_aspect_ratio=True,
                ensure_multiple_of=14,
                resize_method='lower_bound',
                image_interpolation_method=cv2.INTER_CUBIC,
            ),
            NormalizeImage(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),
            PrepareForNet(),
        ] + ([Crop(size[0])] if self.mode == 'train' else []))
        
        # ä»…åœ¨è®ºæ–‡ä¸­æœ‰æ°´å¹³ç¿»è½¬
        self.augs = A.Compose([
            A.HorizontalFlip(),
            A.ColorJitter(hue = 0.1, contrast=0.1, brightness=0.1, saturation=0.1),
            A.GaussNoise(var_limit=25),
        ])
    
    def __getitem__(self, item):
        path = self.paths[item]
        image, depth = self.h5_loader(path)
        
        if self.mode == 'train':
            augmented = self.augs(image=image, mask = depth)
            image = augmented["image"] / 255.0
            depth = augmented['mask']
        else:
            image = image / 255.0
          
        sample = self.transform({'image': image, 'depth': depth})

        sample['image'] = torch.from_numpy(sample['image'])
        sample['depth'] = torch.from_numpy(sample['depth'])
        
        # æœ‰æ—¶æ•°æ®é›†ä¸­ç”±äºå™ªå£°ç­‰åŸå› ï¼Œæ·±åº¦å›¾ä¸­ä¼šæœ‰æœ‰æ•ˆçš„æ·±åº¦æ©ç ã€‚
#         sample['valid_mask'] =... 
     
        return sample

    def __len__(self):
        return len(self.paths)
    
    def h5_loader(self, path):
        h5f = h5py.File(path, "r")
        rgb = np.array(h5f['rgb'])
        rgb = np.transpose(rgb, (1, 2, 0))
        depth = np.array(h5f['depth'])
        return rgb, depth
```

è¿™é‡Œæœ‰å‡ ç‚¹éœ€è¦æ³¨æ„ï¼š
- åŸå§‹çš„ NYU-D æ•°æ®é›†åŒ…å« 40.7 ä¸‡ä¸ªæ ·æœ¬ï¼Œä½†æˆ‘ä»¬ä½¿ç”¨çš„æ˜¯ 4 ä¸‡ä¸ªæ ·æœ¬çš„å­é›†ã€‚è¿™å°†ç•¥å¾®å½±å“æœ€ç»ˆæ¨¡å‹çš„è´¨é‡ã€‚
- è®ºæ–‡çš„ä½œè€…ä»…ä½¿ç”¨æ°´å¹³ç¿»è½¬è¿›è¡Œæ•°æ®å¢å¼ºã€‚
- å¶å°”ï¼Œæ·±åº¦å›¾ä¸­çš„æŸäº›ç‚¹å¯èƒ½æ— æ³•æ­£ç¡®å¤„ç†ï¼Œå¯¼è‡´â€œååƒç´ â€ã€‚ä¸€äº›æ•°æ®é›†é™¤äº†å›¾åƒå’Œæ·±åº¦å›¾ä¹‹å¤–ï¼Œè¿˜åŒ…æ‹¬ä¸€ä¸ªæ©ç ï¼Œç”¨äºåŒºåˆ†æœ‰æ•ˆå’Œæ— æ•ˆåƒç´ ã€‚è¿™ä¸ªæ©ç å¯¹äºåœ¨æŸå¤±å’ŒæŒ‡æ ‡è®¡ç®—ä¸­æ’é™¤ååƒç´ æ˜¯å¿…è¦çš„ã€‚

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/Metric%20and%20Relative%20Monocular%20Depth%20Estimation%20An%20Overview.%20Fine-Tuning%20Depth%20Anything%20V2/depth_holes.png" alt="image/png">
<p>å›¾15ï¼šimage/png</p>
</div>

- åœ¨è®­ç»ƒæœŸé—´ï¼Œæˆ‘ä»¬è°ƒæ•´å›¾åƒå¤§å°ï¼Œä½¿è¾ƒå°çš„è¾¹ä¸º 518 åƒç´ ï¼Œç„¶åè¿›è¡Œè£å‰ªã€‚å¯¹äºéªŒè¯ï¼Œæˆ‘ä»¬ä¸è£å‰ªæˆ–è°ƒæ•´æ·±åº¦å›¾çš„å¤§å°ã€‚ç›¸åï¼Œæˆ‘ä»¬å¯¹é¢„æµ‹çš„æ·±åº¦å›¾è¿›è¡Œä¸Šé‡‡æ ·ï¼Œå¹¶åœ¨åŸå§‹åˆ†è¾¨ç‡ä¸‹è®¡ç®—æŒ‡æ ‡ã€‚

#### æ­¥éª¤ 5ï¼šæ•°æ®å¯è§†åŒ–

```python
num_images = 5

fig, axes = plt.subplots(num_images, 2, figsize=(10, 5 * num_images))

train_set = NYU(train_paths, mode='train') 

for i in range(num_images):
    sample = train_set[i*1000]
    img, depth = sample['image'].numpy(), sample['depth'].numpy()

    mean = np.array([0.485, 0.456, 0.406]).reshape((3, 1, 1))
    std = np.array([0.229, 0.224, 0.225]).reshape((3, 1, 1))
    img = img*std+mean

    axes[i, 0].imshow(np.transpose(img, (1,2,0)))
    axes[i, 0].set_title('å›¾åƒ')
    axes[i, 0].axis('off')

    im1 = axes[i, 1].imshow(depth, cmap='viridis', vmin=0)
    axes[i, 1].set_title('çœŸå®æ·±åº¦')
    axes[i, 1].axis('off')
    fig.colorbar(im1, ax=axes[i, 1])
    
plt.tight_layout()

```
<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/Metric%20and%20Relative%20Monocular%20Depth%20Estimation%20An%20Overview.%20Fine-Tuning%20Depth%20Anything%20V2/dataset.png" alt="image/png">
<p>å›¾16ï¼šimage/png</p>
</div>

å¦‚ä½ æ‰€è§ï¼Œå›¾åƒéå¸¸æ¨¡ç³Šä¸”æœ‰å™ªå£°ã€‚å› æ­¤ï¼Œæˆ‘ä»¬æ— æ³•è·å¾—åœ¨ Depth Anything V2 é¢„è§ˆä¸­çœ‹åˆ°çš„ç»†ç²’åº¦æ·±åº¦å›¾ã€‚åœ¨é»‘æ´ä¼ªå½±ä¸­ï¼Œæ·±åº¦ä¸º 0ï¼Œæˆ‘ä»¬ç¨åå°†åˆ©ç”¨è¿™ä¸€äº‹å®æ¥æ©ç›–è¿™äº›ç©ºæ´ã€‚æ­¤å¤–ï¼Œæ•°æ®é›†ä¸­åŒ…å«è®¸å¤šåŒä¸€ä½ç½®å‡ ä¹ç›¸åŒçš„ç…§ç‰‡ã€‚

#### æ­¥éª¤ 6ï¼šå‡†å¤‡æ•°æ®åŠ è½½å™¨

```python
def get_dataloaders(batch_size):
    
    train_dataset = NYU(train_paths, mode='train')
    val_dataset = NYU(val_paths, mode='val')
    
    
    train_dataloader = torch.utils.data.DataLoader(train_dataset, 
                                                  batch_size = batch_size,
                                                  shuffle=True,
                                                  num_workers=4,
                                                  drop_last=True
                                                  )

    val_dataloader = torch.utils.data.DataLoader(val_dataset, 
                                               batch_size = 1, #ç”¨äºæ— å¡«å……çš„åŠ¨æ€åˆ†è¾¨ç‡è¯„ä¼°
                                               shuffle=False,
                                               num_workers=4,
                                               drop_last=True
                                                )
    
    return train_dataloader, val_dataloader

```
#### æ­¥éª¤ 7ï¼šæŒ‡æ ‡è¯„ä¼°

```python
def eval_depth(pred, target):
    assert pred.shape == target.shape

    thresh = torch.max((target / pred), (pred / target))

    d1 = torch.sum(thresh < 1.25).float() / len(thresh)

    diff = pred - target
    diff_log = torch.log(pred) - torch.log(target)

    abs_rel = torch.mean(torch.abs(diff) / target)

    rmse = torch.sqrt(torch.mean(torch.pow(diff, 2)))
    mae = torch.mean(torch.abs(diff))

    silog = torch.sqrt(torch.pow(diff_log, 2).mean() - 0.5 * torch.pow(diff_log.mean(), 2))

    return {'d1': d1.detach(), 'abs_rel': abs_rel.detach(),'rmse': rmse.detach(), 'mae': mae.detach(), 'silog':silog.detach()}

```
æˆ‘ä»¬çš„æŸå¤±å‡½æ•°æ˜¯ SiLogã€‚ä¼¼ä¹åœ¨å¯¹ç»å¯¹æ·±åº¦è¿›è¡Œè®­ç»ƒæ—¶ï¼Œæˆ‘ä»¬åº”è¯¥å¿˜è®°å°ºåº¦ä¸å˜æ€§å’Œå…¶ä»–ç”¨äºç›¸å¯¹æ·±åº¦è®­ç»ƒçš„æŠ€æœ¯ã€‚ç„¶è€Œï¼Œäº‹å®è¯æ˜è¿™å¹¶ä¸å®Œå…¨æ­£ç¡®ï¼Œæˆ‘ä»¬é€šå¸¸ä»ç„¶å¸Œæœ›ä½¿ç”¨ä¸€ç§â€œå°ºåº¦æ­£åˆ™åŒ–â€ï¼Œä½†ç¨‹åº¦è¾ƒè½»ã€‚å‚æ•°Î»=0.5 æœ‰åŠ©äºåœ¨å…¨å±€ä¸€è‡´æ€§å’Œå±€éƒ¨å‡†ç¡®æ€§ä¹‹é—´å–å¾—å¹³è¡¡ã€‚

#### æ­¥éª¤ 8ï¼šå®šä¹‰è¶…å‚æ•°

```python
model_weights_path =  '/kaggle/working/depth_anything_v2_vits.pth' 
model_configs = {
        'vits': {'encoder': 'vits', 'features': 64, 'out_channels': [48, 96, 192, 384]},
        'vitb': {'encoder': 'vitb', 'features': 128, 'out_channels': [96, 192, 384, 768]},
        'vitl': {'encoder': 'vitl', 'features': 256, 'out_channels': [256, 512, 1024, 1024]},
        'vitg': {'encoder': 'vitg', 'features': 384, 'out_channels': [1536, 1536, 1536, 1536]}
    }
model_encoder = 'vits'
max_depth = 10
batch_size = 11
lr = 5e-6
weight_decay = 0.01
num_epochs = 10
warmup_epochs = 0.5
scheduler_rate = 1
load_state = False

state_path = "/kaggle/working/cp"
save_model_path = '/kaggle/working/model'
seed = 42
mixed_precision = 'fp16'
```
æ³¨æ„å‚æ•°â€œ**æœ€å¤§æ·±åº¦**â€ã€‚æˆ‘ä»¬æ¨¡å‹ä¸­çš„æœ€åä¸€å±‚æ˜¯å¯¹æ¯ä¸ªåƒç´ çš„ sigmoid å‡½æ•°ï¼Œäº§ç”Ÿ 0 åˆ° 1 çš„è¾“å‡ºã€‚æˆ‘ä»¬åªéœ€å°†æ¯ä¸ªåƒç´ ä¹˜ä»¥â€œ**æœ€å¤§æ·±åº¦**â€ï¼Œä»¥è¡¨ç¤ºä» 0 åˆ°â€œ**æœ€å¤§æ·±åº¦**â€çš„è·ç¦»ã€‚

#### æ­¥éª¤ 9ï¼šè®­ç»ƒå‡½æ•°

```python
def train_fn():

    set_seed(seed)
    ddp_kwargs = DistributedDataParallelKwargs(find_unused_parameters=True) 
    accelerator = Accelerator(mixed_precision=mixed_precision, 
                              kwargs_handlers=[ddp_kwargs],
                             )
    
    # åœ¨è®ºæ–‡ä¸­ï¼Œä»–ä»¬éšæœºåˆå§‹åŒ–è§£ç å™¨å¹¶ä»…ä½¿ç”¨ç¼–ç å™¨é¢„è®­ç»ƒæƒé‡ã€‚ç„¶åè¿›è¡Œå…¨æ¨¡å‹å¾®è°ƒ
    # ViT-S ç¼–ç å™¨åœ¨è¿™é‡Œ
    model = DepthAnythingV2(**{**model_configs[model_encoder], 'max_depth': max_depth})
    model.load_state_dict({k: v for k, v in torch.load(model_weights_path).items() if 'pretrained' in k}, strict=False)
    
    optim = torch.optim.AdamW([{'params': [param for name, param in model.named_parameters() if 'pretrained' in name], 'lr': lr},
                       {'params': [param for name, param in model.named_parameters() if 'pretrained' not in name], 'lr': lr*10}],
                      lr=lr, weight_decay=weight_decay)
    
    criterion = SiLogLoss() # ä½œè€…çš„æŸå¤±å‡½æ•°
    
    train_dataloader, val_dataloader = get_dataloaders(batch_size)
    
    scheduler = transformers.get_cosine_schedule_with_warmup(optim, len(train_dataloader)*warmup_epochs, num_epochs*scheduler_rate*len(train_dataloader))
    
    model, optim, train_dataloader, val_dataloader, scheduler = accelerator.prepare(model, optim, train_dataloader, val_dataloader, scheduler)
    
    if load_state:
        accelerator.wait_for_everyone()
        accelerator.load_state(state_path)
        
    best_val_absrel = 1000
    
    
    for epoch in range(1, num_epochs):
        
        model.train()
        train_loss = 0
        for sample in tqdm(train_dataloader, disable = not accelerator.is_local_main_process):
            optim.zero_grad()
            
            img, depth = sample['image'], sample['depth']
            
            pred = model(img) 
                                                     # æ©ç 
            loss = criterion(pred, depth, (depth <= max_depth) & (depth >= 0.001))
            
            accelerator.backward(loss)
            optim.step()
            scheduler.step()
            
            train_loss += loss.detach()
            
            
        train_loss /= len(train_dataloader)
        train_loss = accelerator.reduce(train_loss, reduction='mean').item()
        
        
        model.eval()
        results = {'d1': 0, 'abs_rel': 0,'rmse': 0, 'mae': 0, 'silog': 0}
        for sample in tqdm(val_dataloader, disable = not accelerator.is_local_main_process):
            
            img, depth = sample['image'].float(), sample['depth'][0]
            
            with torch.no_grad():
                pred = model(img)
                # åœ¨åŸå§‹åˆ†è¾¨ç‡ä¸‹è¯„ä¼°
                pred = F.interpolate(pred[:, None], depth.shape[-2:], mode='bilinear', align_corners=True)[0, 0]
            
            valid_mask = (depth <= max_depth) & (depth >= 0.001)
            
            cur_results = eval_depth(pred[valid_mask], depth[valid_mask])
            
            for k in results.keys():
                results[k] += cur_results[k]
            

        for k in results.keys():
            results[k] = results[k] / len(val_dataloader)
            results[k] = round(accelerator.reduce(results[k], reduction='mean').item(),3)
        
        accelerator.wait_for_everyone()
        accelerator.save_state(state_path, safe_serialization=False)
        
        if results['abs_rel'] < best_val_absrel:
            best_val_absrel = results['abs_rel']
            unwrapped_model = accelerator.unwrap_model(model)
            if accelerator.is_local_main_process:
                torch.save(unwrapped_model.state_dict(), save_model_path)
        
        accelerator.print(f"epoch_{epoch},  train_loss = {train_loss:.5f}, val_metrics = {results}")
        
# æ³¨æ„ï¼šåœ¨æµ‹è¯•ä¸€ç§é…ç½®æ—¶ï¼Œæˆ‘é‡åˆ°äº†æŸå¤±å˜ä¸º nan çš„é”™è¯¯ã€‚
# é€šè¿‡åœ¨é¢„æµ‹ä¸­æ·»åŠ ä¸€ä¸ªå°çš„epsilon æ¥é˜²æ­¢é™¤ä»¥ 0 å¯ä»¥ä¿®å¤æ­¤é—®é¢˜
```
åœ¨è®ºæ–‡ä¸­ï¼Œä½œè€…éšæœºåˆå§‹åŒ–è§£ç å™¨å¹¶ä»…ä½¿ç”¨ç¼–ç å™¨æƒé‡ã€‚ç„¶åä»–ä»¬å¯¹æ•´ä¸ªæ¨¡å‹è¿›è¡Œå¾®è°ƒã€‚å…¶ä»–å€¼å¾—æ³¨æ„çš„ç‚¹åŒ…æ‹¬ï¼š
- å¯¹è§£ç å™¨å’Œç¼–ç å™¨ä½¿ç”¨ä¸åŒçš„å­¦ä¹ ç‡ã€‚ç¼–ç å™¨çš„å­¦ä¹ ç‡è¾ƒä½ï¼Œå› ä¸ºæˆ‘ä»¬ä¸æƒ³åƒå¯¹éšæœºåˆå§‹åŒ–çš„è§£ç å™¨é‚£æ ·æ˜¾è‘—æ”¹å˜å·²ç»å¾ˆå¥½çš„æƒé‡ã€‚
- ä½œè€…åœ¨è®ºæ–‡ä¸­ä½¿ç”¨äº†å¤šé¡¹å¼è°ƒåº¦å™¨ï¼Œè€Œæˆ‘ä½¿ç”¨äº†å¸¦æœ‰é¢„çƒ­çš„ä½™å¼¦è°ƒåº¦å™¨ï¼Œå› ä¸ºæˆ‘å–œæ¬¢å®ƒã€‚
- åœ¨æ©ç ä¸­ï¼Œå¦‚å‰æ‰€è¿°ï¼Œæˆ‘ä»¬é€šè¿‡ä½¿ç”¨æ¡ä»¶â€œ**depth >= 0.001**â€æ¥é¿å…æ·±åº¦å›¾ä¸­çš„é»‘æ´ã€‚
- åœ¨è®­ç»ƒå‘¨æœŸä¸­ï¼Œæˆ‘ä»¬åœ¨è°ƒæ•´å¤§å°åçš„æ·±åº¦å›¾ä¸Šè®¡ç®—æŸå¤±ã€‚åœ¨éªŒè¯æœŸé—´ï¼Œæˆ‘ä»¬å¯¹é¢„æµ‹è¿›è¡Œä¸Šé‡‡æ ·å¹¶åœ¨åŸå§‹åˆ†è¾¨ç‡ä¸‹è®¡ç®—æŒ‡æ ‡ã€‚
- çœ‹çœ‹æˆ‘ä»¬å¯ä»¥å¤šä¹ˆå®¹æ˜“åœ°ä½¿ç”¨ HF accelerate ä¸ºåˆ†å¸ƒå¼è®¡ç®—åŒ…è£…è‡ªå®šä¹‰ PyTorch ä»£ç ã€‚

#### æ­¥éª¤ 10ï¼šå¯åŠ¨è®­ç»ƒ

```python
# ä½ å¯ä»¥ä½¿ç”¨ 1 ä¸ª GPU è¿è¡Œæ­¤ä»£ç ã€‚åªéœ€å°† num_processes=1
notebook_launcher(train_fn, num_processes=2)
```

æˆ‘ç›¸ä¿¡æˆ‘ä»¬å·²ç»å®ç°äº†é¢„æœŸçš„ç›®æ ‡ã€‚æ€§èƒ½ä¸Šçš„å¾®å°å·®å¼‚å¯å½’å› äºæ•°æ®é›†å¤§å°çš„æ˜¾è‘—å·®å¼‚ï¼ˆ40k ä¸ 400kï¼‰ã€‚è¯·è®°ä½ï¼Œæˆ‘ä»¬ä½¿ç”¨äº† ViT-S ç¼–ç å™¨ã€‚

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/Metric%20and%20Relative%20Monocular%20Depth%20Estimation%20An%20Overview.%20Fine-Tuning%20Depth%20Anything%20V2/metrics5.png" alt="image/png">
<p>å›¾17ï¼šimage/png</p>
</div>

è®©æˆ‘ä»¬å±•ç¤ºä¸€äº›ç»“æœ

```python
model = DepthAnythingV2(**{**model_configs[model_encoder], 'max_depth': max_depth}).to('cuda')
model.load_state_dict(torch.load(save_model_path))

num_images = 10

fig, axes = plt.subplots(num_images, 3, figsize=(15, 5 * num_images))

val_dataset = NYU(val_paths, mode='val') 
model.eval()
for i in range(num_images):
    sample = val_dataset[i]
    img, depth = sample['image'], sample['depth']
    
    mean = torch.tensor([0.485, 0.456, 0.406]).view(3, 1, 1)
    std = torch.tensor([0.229, 0.224, 0.225]).view(3, 1, 1)
   
    with torch.inference_mode():
        pred = model(img.unsqueeze(0).to('cuda'))
        pred = F.interpolate(pred[:, None], depth.shape[-2:], mode='bilinear', align_corners=True)[0, 0]
            
    img = img*std + mean
     
    axes[i, 0].imshow(img.permute(1,2,0))
    axes[i, 0].set_title('å›¾åƒ')
    axes[i, 0].axis('off')

    max_depth = max(depth.max(), pred.cpu().max())
    
    im1 = axes[i, 1].imshow(depth, cmap='viridis', vmin=0, vmax=max_depth)
    axes[i, 1].set_title('çœŸå®æ·±åº¦')
    axes[i, 1].axis('off')
    fig.colorbar(im1, ax=axes[i, 1])
    
    im2 = axes[i, 2].imshow(pred.cpu(), cmap='viridis', vmin=0, vmax=max_depth)
    axes[i, 2].set_title('é¢„æµ‹æ·±åº¦')
    axes[i, 2].axis('off')
    fig.colorbar(im2, ax=axes[i, 2])

plt.tight_layout()
```

<div class="flex justify-center">
    <img src="https://huggingface.co/datasets/hf-vision/course-assets/resolve/main/Metric%20and%20Relative%20Monocular%20Depth%20Estimation%20An%20Overview.%20Fine-Tuning%20Depth%20Anything%20V2/inference.png" alt="image/png">
<p>å›¾18ï¼šimage/png</p>
</div>

éªŒè¯é›†ä¸­çš„å›¾åƒæ¯”è®­ç»ƒé›†ä¸­çš„å›¾åƒæ›´æ¸…æ™°ã€æ›´å‡†ç¡®ï¼Œè¿™å°±æ˜¯ä¸ºä»€ä¹ˆæˆ‘ä»¬çš„é¢„æµ‹ç›¸æ¯”ä¹‹ä¸‹æ˜¾å¾—æœ‰ç‚¹æ¨¡ç³Šçš„åŸå› ã€‚å†çœ‹çœ‹ä¸Šé¢çš„è®­ç»ƒæ ·æœ¬ã€‚

æ€»çš„æ¥è¯´ï¼Œå…³é”®è¦ç‚¹æ˜¯æ¨¡å‹çš„è´¨é‡åœ¨å¾ˆå¤§ç¨‹åº¦ä¸Šå–å†³äºæä¾›çš„æ·±åº¦å›¾çš„è´¨é‡ã€‚Depth Anything V2 çš„ä½œè€…å…‹æœäº†è¿™ä¸€é™åˆ¶å¹¶ç”Ÿæˆäº†éå¸¸æ¸…æ™°çš„æ·±åº¦å›¾ï¼Œå€¼å¾—ç§°èµã€‚å”¯ä¸€çš„ç¼ºç‚¹æ˜¯å®ƒä»¬æ˜¯ç›¸å¯¹æ·±åº¦ã€‚

## å‚è€ƒæ–‡çŒ®
- [Towards Robust Monocular Depth Estimation: Mixing Datasets for Zero-shot Cross-dataset Transfer](https://arxiv.org/pdf/1907.01341)
- [ZoeDepth: Zero-shot Transfer by Combining Relative and Metric Depth](https://arxiv.org/pdf/2302.12288)
- [Vision Transformers for Dense Prediction](https://arxiv.org/pdf/2103.13413)
- [Depth Anything: Unleashing the Power of Large-Scale Unlabeled Data](https://arxiv.org/pdf/2401.10891)
- [Depth Anything V2](https://arxiv.org/pdf/2406.09414)