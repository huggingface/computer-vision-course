# Bienvenid@ al curso comunitario de Visión Artificial

Querido participante, 

Bienvenid@ al **curso de visión artificial impulsado por la comunidad de 🤗**. El campo de la visión artificial está revolucionando el mundo de muchas maneras, desde el desbloqueo de teléfonos móviles a través de sistemas de reconocimiento facial, pasando por el análisis de imágenes médicas para la detección de enfermedades, hasta la vigilancia de la vida salvaje o la generación de nuevas imágenes. Juntos, nos sumergiremos en el fascinante mundo de la visión artificial!

En este curso cubriremos todo lo relacionado con visión artificial, desde los conceptos más básicos hasta los últimos avances en este campo. Está estructurado y diseñado para incluir temas fundacionales, de tal manera que sea accesible y agradable para todo el mundo. Nos encanta que hayas decidido acompañarnos en este fantástico viaje!

En esta página encontrarás información sobre cómo unirte a la comunidad de participantes, subir tus archivos, obtener la certificación, y otros detalles sobre el curso!

## Tareas 📄

Para obtener la certificación al terminar el curso, tendrás que realizar las siguientes tareas:

1. Entrenar/afinar un modelo
2. Crear una aplicación y subirla/hostearla en un espacio de Hugging Face (Hugging Face Spaces)

### Entrenar/Afinar un modelo

Para ello, encontrarás notebooks debajo de la sección de Notebooks/Vision Transformers. De momento, existen notebooks para detección de objetos, segmentación de imágenes y clasificación de imágenes. Puedes tanto entrenar un modelo en un dataset que ya exista en el repositorio de 🤗 o subir un dataset a un repositorio privado y entrenar un modelo sobre ese mismo dataset.

El repositorio de modelos necesita contar con lo siguiente:


1. Una tarjeta de modelo rellenada correctamente, puedes dirigirte [aquí para más información sobre como rellenarla](https://huggingface.co/docs/hub/en/model-cards).
2. Si has entrenado un modelo utilizando la librería de transformers y lo has subido al repositorio de HF, la tarjeta del modelo se generará automáticamente. En ese caso, edita la tarjeta y rellena los detalles que faltan.
3. Añade el ID del dataset a la tarjeta del modelo para enlazar el repositorio del modelo al repositorio del dataset.

### Creando un Espacio

En esta parte de la tarea construirás una aplicación basada en Gradio para tu modelo de visión artificial, la cual compartirás posteriormente en los espacios de 🤗. Explora más info sobre estas tareas en los siguientes recursos:

- [Empezando con Gradio](https://huggingface.co/learn/nlp-course/chapter9/1?fw=pt#introduction-to-gradio)
- [Cómo compartir tu aplicación en los espacios de 🤗](https://huggingface.co/learn/nlp-course/chapter9/4?fw=pt)

## Certificación 🥇

Una vez que hayas finalizado las tareas de Entrenar/Afinar un modelo y crear un espacio, por favor, rellena el [formulario](https://forms.gle/isiVSw59oiiHP6pN9) con tu nombre, correo, y links a los repositorios de tu modelo y espacio para recibir tu certificado.

## Únete a la comunidad!

Te invitamos a ser parte de [nuestra comunidad activa y amistosa en Discord](http://hf.co/join/discord), donde nació este curso y donde podrás encontrar conversaciones fascinantes e intereses comunes con otros miembros. Allí encontrarás otros participantes con los que intercambiar ideas y recursos. Es tu fuente para colaborar con otros, recibir feedback y preguntar dudas!

También es una manera de motivarte para seguir el curso. Unirse a nuestra comunidad es una manera estupenda de mantenerse al día. Quién sabe que será lo próximo que construiremos juntos?

Mientras la Inteligencia Artificial continúa avanzando, también lo hace la calidad de nuestras discusiones y la diversidad de las perspectivas dentro de nuestra comunidad. Al convertirte en miembro, tendrás la oportunidad de conectar con otros participantes del curso, intercambiar ideas, y colaborar con otros. Además, los creadores y colaboradores de este curso están atentos y activos en Discord, y podrían ayudarte en caso de que lo necesitases. Únete ahora!

## Canales de Visión Artificial

Hay muchos canales en nuestro servidor de Discord, cada uno dedicado a temas distintos. Te encontrarás a gente hablando de artículos académicos, organizando eventos, compartiendo sus proyectos e ideas, compartiendo ideas y haciendo brainstorming y mucho más.

Como participante del curso de Visión Artificial, puede que encuentres especialmente relevantes los siguientes canales:

- `#computer-vision`: un canal general para todo lo relacionado con Visión Artificial
- `#3d`: un canal para hablar de Visión Artificial específicamente aplicada a Visión Artificial en 3D.

Si estás interesado en Inteligencia Artificial Generativa, también te invitamos a que te unas a todos los canales relacionados con modelos de Difusión: `#core-announcements`, `#discussions`, `#dev-discussions`, y `#i-made-this`.

## Qué aprenderás

Este curso está compuesto de teoría, tutoriales prácticos y retos fascinantes.

- **Parte de teoría**: Esta sección cubre los principios teóricos de Visión Artificial, todos ellos explicados en detalle con ejemplos prácticos.
- **Tutoriales prácticos**: Aprenderás cómo entrenar y aplicar modelos relevantes de Visión Artificial utilizando notebooks de Google Collab.

A lo largo de este curso, cubriremos todo, desde los conceptos básicos a los últimos avances en Visión Artificial. Está estructurado de manera que cubre varios temas fundacionales, dándote una perspectiva y conocimiento en profundidad de por qué el uso de técnicas de Visión Artificial tiene tanto impacto en el mundo real.

## Prerrequisitos

Antes de empezar el curso, asegúrate de contar con algo de experiencia programando en Python y conocer conceptos relacionados con transformers, machine learning y redes neuronales. Si cualquiera de estos conceptos es nuevo para ti, quizá sea bueno que repases [la primera unidad del curso de NLP de Hugging Face](https://huggingface.co/learn/nlp-course/chapter1/3?fw=pt). Tener un buen conocimiento de técnicas de preprocesamiento y operaciones matemáticas es bueno, no son necesariamente prerrequisitos para este curso.

## Estructura del curso

El curso está organizado en varias unidades, las cuales van desde lo fundamental hasta un repaso profundo de los modelos más avanzados (state-of-the-art models o SOTA):

- **Unidad 1 - Conceptos fundamentales de Visión Artificial** : esta unidad cubre los conceptos esenciales para empezar con visión artificial: la necesidad para la existencia de visión artificial, los conceptos básicos de este campo y sus aplicaciones. También se exploran aspectos como la formación, el preprocesamiento y otros conceptos básicos del tratamiento de imágenes, así como aspectos clave de la extracción de características.
- **Unidad 2 - Redes Neuronales Convolucionales (CNNs por sus siglas en inglés)** : explora el mundo de las CNNs, entendiendo su arquitectura general, conceptos clave y los modelos pre-entrenados más relevantes. Aprende como aplicar conocimiento por transferencia (transfer learning) y afinar este tipo de modelos pre-entrenados para adaptarlos a tareas específicas. 
- **Unidad 3 - Transformers para Visión (Vision Transformers o ViTs por sus siglas en inglés)** : explora la arquitectura de transformers en un contexto de visión artificial y aprende como se comparan con las redes convolucionales. Aprende sobre los transformers de visión más utilizados como Swin, DETR o CVT, junto con técnicas de transfer learning y el afinamiento de dichos modelos.
- **Unidad 4 - Modelos multimodales** : entiende la mezcla entre texto y visión a través de la exploración de tareas multimodales como imagen-a-texto y texto-a-imagen. Estudia modelos como CLIP y aquellos relacionados (GroupViT, BLIPM, Owl-VIT), y domina las técnicas de transfer learning para tareas multimodales.
- **Unidad 5 - Modelos generativos** : explora modelos generativos, incluyendo GANs (redes generativas adversativas), VAEs (autoencoders variacionales) y modelos de Difusión. Aprende sus diferencias y aplicaciones en tareas como texto-a-imagen, imagen-a-imagen e inpainting (restauración o generación de partes en una imagen).
- **Unidad 6 - Tareas básicas de Visión Artificial** : cubriremos tareas fundamentales como clasificación de imágenes, detección de objetos y segmentación de imágenes, y los modelos más habituales utilizados para estas tareas (YOLO, SAM). Aprenderás sobre las métricas de evaluación y las aplicaciones reales de estas tareas. 
- **Unidad 7 - Video y procesamiento de video** : examinaremos las características de los videos, el rol del procesamiento de video y los retos que existen en comparación al procesamiento de imágenes. Exploraremos continuidad temporal, estimación del movimiento y las aplicaciones prácticas del procesamiento de video.
- **Unidad 8 - Visión para 3D, Renderización y reconstrucción de escenas** : explora las complejidades de la visión en tres dimensiones, explorando conceptos como Nerf y GQN para renderización y reconstrucción de escenas. También aprenderás sobre los retos y aplicaciones de la visión artificial en 3D y como provee de una perspectiva con incluso mayor información espacial.
- **Unidad 9 - Optimización de modelos** : explora los aspectos críticos de la optimización de modelos. Cubriremos técnicas como compresión de modelos, consideraciones a la hora del despliegue y, por último, el uso de herramientas y frameworks. Se incluyen temas como destilación (distillation), poda (pruning) y TinyML para un despliegue de modelos eficiente.
- **Unidad 10 - Creación de datos sintéticos** : descubre la importancia de la creación de datos sintéticos utilizando modelos generativos. Explora métodos como puntos de nubes y modelos de Difusión e investiga datasets sintéticos relevantes y sus aplicaciones en visión artificial.
- **Unidad 11 - Visión Artificial de Zero Shot** : entra en el reino del aprendizaje zero-shot en visión artificial, cubriendo aspectos de generalización, transfer learning y sus aplicaciones en tareas como reconocimiento de zero-shot y segmentación de imágenes. Explora la relación entre el aprendizaje zero-shot y transfer learning de manera transversal en distintas áreas de Visión Artificial.
- **Unidad 12 - Ética y sesgo en Visión Artificial** : entiende las consideraciones éticas específicas a Visión Artificial. Explora por qué la ética importa en Visión Artificial, cómo los sesgos pueden existir en modelos de Inteligencia Artificial y los tipos de sesgos que prevalecen en estos campos. Aprende cómo evaluar y mitigar el sesgo, siempre enfatizando el desarrollo y despliegue responsable de tecnologías de Inteligencia Artificial.
- **Unidad 13 - Futuro y tendencias emergentes** : explora las tendencias actuales y las arquitecturas emergentes. Entra en enfoques novedosos como las Redes Retentiva, Hiera, Hyena, I-JEPA y los modelos de visión Retentivos.

## Conoce a nuestro equipo

Este curso está hecho por la Comunidad de Hugging Face con 💜! Únete a nosotros añadiendo tu contribución [en GitHub](https://github.com/johko/computer-vision-course).
Nuestra meta era crear un curso de visión artificial que es fácil para principiantes y que se pueda utilizar como un recurso para otros. Más de 60 personas de todo el mundo han unido sus fuerzas para que este proyecto pasase. Aquí les damos crédito:

**Unidad 1 - Conceptos fundamentales de Visión Artificial**

- Revisores: [Ratan Prasad](https://github.com/ratan), [Ameed Taylor](https://github.com/atayloraerospace), [Sergio Paniego](https://github.com/sergiopaniego)
- Escritores: [Seshu Pavan Mutyala](https://github.com/seshupavan), [Isabella Bicalho-Frazeto](https://github.com/bellabf), [Aman Kapoor](https://github.com/aman06012003), [Tiago Comassetto Fróes](https://github.com/froestiago), [Aditya Mishra](https://github.com/adityaiiitr), [Kerem Delikoyun](https://github.com/krmdel), [Ker Lee Yap](https://github.com/klyap), [Kathy Fahnline](https://github.com/kfahn22), [Ameed Taylor](https://github.com/atayloraerospace)

**Unidad 2 - Redes Neuronales Convolucionales (CNNs)**

- Revisores: [Ratan Prasad](https://github.com/ratan), [Mohammed Hamdy](https://github.com/mmhamdy), [Sezan](https://github.com/sezan92), [Joshua Adrian Cahyono](https://github.com/JvThunder), [Murtaza Nazir](https://github.com/themurtazanazir), [Albert Kao](https://github.com/albertkao227), [Sitam Meur](https://github.com/sitamgithub-MSIT), [Antonis Stellas](https://github.com/AntonisCSt), [Sergio Paniego](https://github.com/sergiopaniego)
- Escritores: [Emre Albayrak](https://github.com/emre570), [Caroline Shamiso Chitongo](https://github.com/ShamieCC), [Sezan](https://github.com/sezan92), [Joshua Adrian Cahyono](https://github.com/JvThunder), [Murtaza Nazir](https://github.com/themurtazanazir), [Albert Kao](https://github.com/albertkao227), [Isabella Bicalho-Frazeto](https://github.com/bellabf), [Aman Kapoor](https://github.com/aman06012003), [Sitam Meur](https://github.com/sitamgithub-MSIT)

**Unidad 3 - Transformers para Visión (ViTs)** 

- Revisores: [Ratan Prasad](https://github.com/ratan), [Mohammed Hamdy](https://github.com/mmhamdy), [Ameed Taylor](https://github.com/atayloraerospace), [Sezan](https://github.com/sezan92)
- Escritores: [Surya Guthikonda](https://github.com/SuryaKrishna02), [Ker Lee Yap](https://github.com/klyap), [Anindyadeep Sannigrahi](https://bento.me/anindyadeep), [Celina Hanouti](https://github.com/hanouticelina), [Malcolm Krolick](https://github.com/Mkrolick), [Alvin Li](https://github.com/alvanli), [Shreyas Daniel Gaddam](https://shreydan.github.io), [Anthony Susevski](https://github.com/asusevski), [Alan Ahmet](https://github.com/alanahmet), [Ghassen Fatnassi](https://github.com/ghassen-fatnassi)

**Unidad 4 - Modelos multimodales**

- Revisores: [Ratan Prasad](https://github.com/ratan), [Snehil Sanyal](https://github.com/snehilsanyal), [Mohammed Hamdy](https://github.com/mmhamdy), [Charchit Sharma](https://github.com/charchit7), [Ameed Taylor](https://github.com/atayloraerospace), [Isabella Bicalho-Frazeto](https://github.com/bellabf)
- Escritores: [Snehil Sanyal](https://github.com/snehilsanyal), [Surya Guthikonda](https://github.com/SuryaKrishna02), [Mateusz Dziemian](https://github.com/mattmdjaga), [Charchit Sharma](https://github.com/charchit7), [Evstifeev Stepan](https://github.com/minemile), [Jeremy Kespite](https://github.com/jeremy-k3/), [Isabella Bicalho-Frazeto](https://github.com/bellabf), [Pedro Gabriel Gengo Lourenco](https://github.com/pedrogengo)

**Unidad 5 - Modelos generativos** 

- Revisores: [Ratan Prasad](https://github.com/ratan), [William Bonvini](https://github.com/WilliamBonvini), [Mohammed Hamdy](https://github.com/mmhamdy), [Ameed Taylor](https://github.com/atayloraerospace)-
- Escritores: [Jeronim Matijević](github.com/jere357), [Mateusz Dziemian](https://github.com/mattmdjaga), [Charchit Sharma](https://github.com/charchit7), [Muhammad Waseem](https://github.com/hwaseem04)

**Unidad 6 - Tareas básicas de Visión Artificial** 

- Revisores: [Adhi Setiawan](https://github.com/adhiiisetiawan)
- Escritores: [Adhi Setiawan](https://github.com/adhiiisetiawan), [Bastien Pouëssel](https://github.com/Skower)

**Unidad 7 - Video y procesamiento de video**

- Revisores: [Ameed Taylor](https://github.com/atayloraerospace), [Isabella Bicalho-Frazeto](https://github.com/bellabf)
- Escritores: [Diwakar Basnet](https://github.com/DiwakarBasnet), [Chulhwa Han](https://github.com/cjfghk5697), [Woojun Jung](https://github.com/jungnerd), [Jiwook Han](https://github.com/mreraser), [Mingi Kim](https://github.com/1kmmk1)

**Unidad 8 - Visión para 3D, Renderización y reconstrucción de escenas** 

- Revisores: [Ratan Prasad](https://github.com/ratan), [William Bonvini](https://github.com/WilliamBonvini), [Mohammed Hamdy](https://github.com/mmhamdy), [Adhi Setiawan](https://github.com/adhiiisetiawan), [Ameed Taylor](https://github.com/atayloraerospace0)
- Escritores: [John Fozard](https://github.com/jfozard), [Vasu Gupta](https://github.com/vasugupta9), [Psetinek](https://github.com/psetinek)

**Unidad 9 - Optimización de modelos**

- Revisores: [Ratan Prasad](https://github.com/ratan), [Mohammed Hamdy](https://github.com/mmhamdy), [Adhi Setiawan](https://github.com/adhiiisetiawan), [Ameed Taylor](https://github.com/atayloraerospace)
- Escritores: [Adhi Setiawan](https://github.com/adhiiisetiawan)

**Unidad 10 - Creación de datos sintéticos**

- Revisores: [Mohammed Hamdy](https://github.com/mmhamdy), [Ameed Taylor](https://github.com/atayloraerospace), [Bhavesh Misra](https://github.com/Zekrom-7780)
- Escritores: [William Bonvini](https://github.com/WilliamBonvini), [Alper Balbay](https://github.com/alperiox), [Madhav Kumar](https://github.com/miniMaddy), [Bhavesh Misra](https://github.com/Zekrom-7780), [Kathy Fahnline](https://github.com/kfahn22)

**Unidad 11 - Visión Artificial de Zero Shot**

- Revisores: [Ratan Prasad](https://github.com/ratan), [Mohammed Hamdy](https://github.com/mmhamdy), [Albert Kao](https://github.com/albertkao227), [Isabella Bicalho-Frazeto](https://github.com/bellabf)
- Escritores: [Mohammed Hamdy](https://github.com/mmhamdy), [Albert Kao](https://github.com/albertkao227)

**Unidad 12 - Ética y sesgo en Visión Artificial**

- Revisores: [Ratan Prasad](https://github.com/ratan), [Mohammed Hamdy](https://github.com/mmhamdy), [Charchit Sharma](https://github.com/charchit7), [Adhi Setiawan](https://github.com/adhiiisetiawan), [Ameed Taylor](https://github.com/atayloraerospace), [Bhavesh Misra](https://github.com/Zekrom-7780)
- Escritores: [Snehil Sanyal](https://github.com/snehilsanyal), [Bhavesh Misra](https://github.com/Zekrom-7780)

**Unidad 13 - Futuro y tendencias emergentes**

- Revisores: [Ratan Prasad](https://github.com/ratan), [Ameed Taylor](https://github.com/atayloraerospace), [Mohammed Hamdy](https://github.com/mmhamdy)
- Escritores: [Farros Alferro](https://github.com/farrosalferro), [Mohammed Hamdy](https://github.com/mmhamdy), [Louis Ulmer](https://github.com/lulmer), [Dario Wisznewer](https://github.com/dariowsz), [gonzachiar](https://github.com/gonzachiar)

**Equipo de organización**
[Merve Noyan](https://github.com/merveenoyan), [Adam Molnar](https://github.com/lunarflu), [Johannes Kolbe](https://github.com/johko)

Nos encanta tenerte aquí, vamos a empezar!
